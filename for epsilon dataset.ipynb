{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "little-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PU ExtraTree - A DT Classifier for PU Learning\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.sparse\n",
    "\n",
    "class PUExtraTree:\n",
    "    def __init__(self, risk_estimator = \"nnPU\",\n",
    "                 loss = \"quadratic\",\n",
    "                 max_depth = None,\n",
    "                 min_samples_leaf = 1,\n",
    "                 max_features = \"sqrt\",\n",
    "                 max_candidates = 1):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        risk_estimator : {\"PN\", \"uPU\", \"nnPU\"}, default='nnPU'\n",
    "            PU data based risk estimator. Supports supervised (PN) learning, unbiased PU (uPU) learning and nonnegative PU (nnPU) learning.\n",
    "        loss : {\"quadratic\", \"logistic\"}, default='quadratic'\n",
    "            The function to measure the cost of making an incorrect prediction. Supported loss functions are:\n",
    "            \"quadratic\" l(v,y) = (1-vy)^2 and\n",
    "            \"logistic\" l(v,y) = ln(1+exp(-vy)).\n",
    "        max_depth : int or None, default=None\n",
    "            The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_leaf samples.\n",
    "        min_samples_leaf : int, default=1\n",
    "            The minimum number of samples required to be at a leaf node. The default is 1.\n",
    "        max_features : int or {\"sqrt\", \"all\"}, default=\"sqrt\"\n",
    "            The number of features to consider when looking for the best split. If \"sqrt\", then max_features = ceil(sqrt(n_features)). If \"all\", then max_features = n_features.\n",
    "        max_candidates : int, default=1\n",
    "            Number of randomly chosen split points to consider for each candidate feature.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.risk_estimator = risk_estimator\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.max_candidates = max_candidates\n",
    "\n",
    "        self.is_trained = False # indicate if tree empty/trained\n",
    "        self.leaf_count = 0\n",
    "        self.current_max_depth = 0\n",
    "        self.nodes = {(0,0): {'data': None, 'j': None, 'xi': None, 'g': None,\n",
    "                              'is_leaf': None, 'risk_reduction': None}}\n",
    "\n",
    "\n",
    "    def create_successor(self, node, side):\n",
    "        \"\"\"\n",
    "        Create an empty child node (either T or F) in the tree.\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : tuple of length 2.\n",
    "            The parent node. First element is the depth in the tree, second element is the position at that depth.\n",
    "        side : {\"T\", \"F\"} or {\"L\", \"R\"}\n",
    "            Whether the node corresponds to a True or False split.\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        row, column = node\n",
    "        if side in ['T','L']:\n",
    "            self.nodes[(row+1, 2*column)] = {'data': None, 'j': None,\n",
    "                                                   'xi': None, 'g': None,\n",
    "                                                   'is_leaf': None, 'loss': None,\n",
    "                                                   'risk_reduction': None}\n",
    "        elif side in ['F','R']:\n",
    "            self.nodes[(row+1, 2*column+1)] = {'data': None, 'j': None,\n",
    "                                                     'xi': None, 'g': None,\n",
    "                                                     'is_leaf': None, 'loss': None,\n",
    "                                                     'risk_reduction': None}\n",
    "        elif side not in ['T','F','L','R']:\n",
    "            print('choose valid position of child node: \\'L\\', \\'R\\', \\'T\\', \\'F\\'')\n",
    "\n",
    "\n",
    "\n",
    "    def get_parent(self, node, return_truth_val):\n",
    "        \"\"\"\n",
    "        Return parent node, and optionally the relationship to child node (T/F).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : tuple of length 2\n",
    "            The child node.\n",
    "        return_truth_val : bool\n",
    "            Indicate whether the truth value should also be returned, that is, whether the child node corresponds to a true or false split.\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of length 2 or (tuple of length 2, bool)\n",
    "            The parent node, optionally the relationships to the parent nodes.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        parent = (node[0] - 1, node[1] // 2)\n",
    "        if return_truth_val:\n",
    "            if node[1] % 2 == 0:\n",
    "                return parent, True\n",
    "            else:\n",
    "                return parent, False\n",
    "        else:\n",
    "            return parent\n",
    "\n",
    "\n",
    "\n",
    "    def get_ancestory(self, node):\n",
    "        \"\"\"\n",
    "        Get parent nodes and relationship to the child nodes all the way to the root.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : tuple of length 2\n",
    "            Child node.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List of nodes.\n",
    "        bools : list\n",
    "            List of bools with the relationships to the parents.\n",
    "\n",
    "        \"\"\"\n",
    "        chain = [node]\n",
    "        bools = []\n",
    "        while chain[-1] != (0,0):\n",
    "            parent, relationship = self.get_parent(chain[-1], True)\n",
    "            chain += [parent]\n",
    "            bools += [relationship]\n",
    "\n",
    "        return chain[1:], bools\n",
    "\n",
    "\n",
    "    def load_tree(self, nodes):\n",
    "        \"\"\"\n",
    "        Load saved tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nodes : dictionary\n",
    "            Dictionary describing the trained decision tree. Typically output from self.nodes.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.nodes = nodes\n",
    "        self.is_trained = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, pi, P = None, U = None, N = None):\n",
    "        \"\"\"\n",
    "        Fit the decision tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pi : float\n",
    "            Prior probability that an example belongs to the positive class.\n",
    "        P : array-like of shape (n_p, n_features), default=None\n",
    "            Training samples from the positive class.\n",
    "        U : array-like of shape (n_u, n_features), default=None\n",
    "            Unlabeled training samples.\n",
    "        N : array-like of shape (n_n, n_features), default=None\n",
    "            Training samples from the negative class if performing PN learning.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "            Returns instance of self.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.risk_estimator in ['uPU', 'nnPU']:\n",
    "            X = np.concatenate((P, U), axis = 0)\n",
    "            y = np.concatenate((np.ones(len(P)), np.zeros(len(U))))\n",
    "        elif self.risk_estimator in ['PN']:\n",
    "            X = np.concatenate((P, N), axis = 0)\n",
    "            y = np.concatenate((np.ones(len(P)), -np.ones(len(N))))\n",
    "\n",
    "        # X = X.astype(np.float32)\n",
    "        y = y.astype(np.int8).flatten()\n",
    "        n, self.d = X.shape\n",
    "        n_p = (y == 1).sum()\n",
    "        n_u = (y == 0).sum()\n",
    "        n_n = (y == -1).sum()\n",
    "        self.pi = pi\n",
    "\n",
    "        if self.pi is None:\n",
    "            print('please specify pi')\n",
    "\n",
    "        if self.max_features == 'sqrt':\n",
    "            self.max_features = int(np.ceil(np.sqrt(X.shape[1])))\n",
    "        elif self.max_features == 'all':\n",
    "            self.max_features = X.shape[1]\n",
    "        elif self.max_features in [i for i in range(1, self.d+1)]:\n",
    "            None\n",
    "        else:\n",
    "            print('select valid number of max features to consider splitting on.')\n",
    "            return None\n",
    "\n",
    "\n",
    "        self.nodes[(0,0)]['data'] = scipy.sparse.coo_matrix(np.ones(n).astype(bool))\n",
    "\n",
    "        def data_at_node(node):\n",
    "            # return subset of training data in partition specified by certain node\n",
    "\n",
    "            if self.nodes[node]['data'] is not None:\n",
    "                return self.nodes[node]['data'].toarray()[0]\n",
    "            else:\n",
    "                # get indices of data at parent\n",
    "                parent_node, relationship = self.get_parent(node, True)\n",
    "                ind_parent = self.nodes[parent_node]['data'].toarray()[0].copy()\n",
    "                checks = (X[ind_parent, self.nodes[parent_node]['j']] <= self.nodes[parent_node]['xi']) == relationship\n",
    "                ind_parent[ind_parent] = checks.flatten()\n",
    "                self.nodes[node]['data'] = scipy.sparse.coo_matrix(ind_parent)\n",
    "                return ind_parent\n",
    "\n",
    "\n",
    "        def impurity_node(y_sigma):\n",
    "            # impurity of single node\n",
    "            if self.risk_estimator in [\"uPU\", \"nnPU\"]:\n",
    "                Wp = (y_sigma == 1).sum() * self.pi/n_p\n",
    "                Wn = (y_sigma == 0).sum()/n_u - Wp\n",
    "\n",
    "                if Wp + Wn == 0:\n",
    "                    vstar = float('inf')\n",
    "                else:\n",
    "                    vstar = Wp/(Wp + Wn)\n",
    "\n",
    "            elif self.risk_estimator in ['PN']:\n",
    "                Wp = (y_sigma == 1).sum() * self.pi/n_p\n",
    "                Wn = (y_sigma == -1).sum() * (1-self.pi)/n_n\n",
    "\n",
    "                if Wp + Wn == 0:\n",
    "                    vstar = float('inf')\n",
    "                else:\n",
    "                    vstar = Wp/(Wp + Wn)\n",
    "\n",
    "\n",
    "            if self.loss == \"quadratic\":\n",
    "                if self.risk_estimator == \"uPU\" and vstar == float('inf'):\n",
    "                    return -float('inf')\n",
    "                elif self.risk_estimator == \"nnPU\" and vstar > 1:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 4 * (Wp + Wn) * vstar * (1 - vstar)\n",
    "\n",
    "            elif self.loss == \"logistic\":\n",
    "                if self.risk_estimator == \"uPU\" and vstar > 1:\n",
    "                    return -float('inf')\n",
    "                elif self.risk_estimator in [\"uPU\", \"nnPU\", \"PN\"] and vstar in [0,1]:\n",
    "                    return 0\n",
    "                elif self.risk_estimator == \"nnPU\" and vstar > 1:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return (Wp + Wn) * (-vstar*np.log(vstar) - (1-vstar)*np.log(1-vstar))\n",
    "\n",
    "\n",
    "        def impurity_split(sigma, j, xi):\n",
    "            mask = (X[sigma, j] <= xi).flatten()\n",
    "            imT = impurity_node(y[sigma][mask])\n",
    "            imF = impurity_node(y[sigma][~mask])\n",
    "            return imT + imF\n",
    "\n",
    "\n",
    "        def regional_prediction_function(y_sigma):\n",
    "            if self.risk_estimator in [\"uPU\", \"nnPU\"]:\n",
    "                Wp = (y_sigma == 1).sum() * self.pi/n_p\n",
    "                Wn = (y_sigma == 0).sum()/n_u - Wp\n",
    "\n",
    "                if Wp + Wn == 0:\n",
    "                    vstar = float('inf')\n",
    "                else:\n",
    "                    vstar = Wp/(Wp + Wn)\n",
    "\n",
    "            elif self.risk_estimator in [\"PN\"]:\n",
    "                Wp = (y_sigma == 1).sum() * self.pi/n_p\n",
    "                Wn = (y_sigma == -1).sum() * (1-self.pi)/n_n\n",
    "\n",
    "                if Wp + Wn == 0:\n",
    "                    vstar = float('inf')\n",
    "                else:\n",
    "                    vstar = Wp/(Wp + Wn)\n",
    "\n",
    "            if vstar > 0.5:\n",
    "                return 1\n",
    "            elif vstar < 0.5:\n",
    "                return -1\n",
    "            elif vstar == 0.5:\n",
    "                return 2*np.random.binomial(1,0.5)-1\n",
    "\n",
    "\n",
    "        def construct_subtree(node, sigma):\n",
    "            # first check stopping criteria\n",
    "            impurity = impurity_node(y[sigma])\n",
    "\n",
    "            # check node pure\n",
    "            if self.risk_estimator in ['nnPU', 'PN']:\n",
    "                c1 = impurity > 0\n",
    "            elif self.risk_estimator == 'uPU':\n",
    "                if y[sigma].sum() == 0:\n",
    "                    c1 = impurity > 0\n",
    "                else:\n",
    "                    c1 = impurity > -float('inf')\n",
    "\n",
    "            # check max depth reached\n",
    "            if self.max_depth is None:\n",
    "                c2 = True\n",
    "            else:\n",
    "                c2 = node[0] < self.max_depth # max depth reached\n",
    "            c3 = self.min_samples_leaf < sigma.sum() # minimum samples in node reached\n",
    "            att_ptp = np.ptp(X[sigma], axis = 0)\n",
    "            c4 = att_ptp.sum() > 0 # check if there is any variability in features\n",
    "            # c4 = np.unique(X[sigma], axis = 0).shape[0] > 1\n",
    "            # check if any of the criteria satisfied\n",
    "            # if so, turn into a leaf node\n",
    "            if c1*c2*c3*c4 == 0:\n",
    "                self.nodes[node]['is_leaf'] = True\n",
    "                lab = regional_prediction_function(y[sigma])\n",
    "                self.nodes[node]['g'] = lab\n",
    "                self.nodes[node]['risk_reduction'] = 0\n",
    "                self.leaf_count += 1\n",
    "            else:\n",
    "                self.nodes[node]['is_leaf'] = False\n",
    "                # find valid nodes that can be used for a split\n",
    "                atts = []\n",
    "                for i in range(self.d):\n",
    "                    if att_ptp[i] > 0:\n",
    "                        atts += [i]\n",
    "\n",
    "                # ranomly pick candiates attributes\n",
    "                attributes = np.random.choice(atts, size = min(self.max_features, len(atts)), replace = False)\n",
    "                candidates = []\n",
    "                candidate_attributes = []\n",
    "                candidate_cut_points = []\n",
    "                for i in range(len(attributes)):\n",
    "                    for j in range(self.max_candidates):\n",
    "                        # need to guard against errors caused by finite precision\n",
    "                        a_,b_,c_,d_ = np.unique(X[sigma, attributes[i]])[[0,1,-2,-1]]\n",
    "                        cut_point = np.random.uniform(a_ + 2*(b_-a_)/5, c_ + 3*(d_-c_)/5)\n",
    "                        candidates += [[attributes[i], cut_point]]\n",
    "                        candidate_attributes += [attributes[i]]\n",
    "                        candidate_cut_points += [cut_point]\n",
    "\n",
    "                impurities = []\n",
    "                for i in range(len(candidates)):\n",
    "                    impurities += [impurity_split(sigma, candidate_attributes[i], candidate_cut_points[i])]\n",
    "\n",
    "                minimiser = np.argmin(impurities)\n",
    "                best_attribute = candidate_attributes[minimiser]\n",
    "                best_cut_point = candidate_cut_points[minimiser]\n",
    "\n",
    "                self.nodes[node]['j'] = int(best_attribute)\n",
    "                self.nodes[node]['xi'] = best_cut_point\n",
    "                self.nodes[node]['risk_reduction'] = impurity - impurities[minimiser]\n",
    "\n",
    "                # create successors of current node\n",
    "                self.create_successor(node, 'T')\n",
    "                self.create_successor(node, 'F')\n",
    "\n",
    "                # get set of data in these successors\n",
    "                succs = ((node[0]+1, 2*node[1]), (node[0]+1, 2*node[1]+1))\n",
    "                sigma_T = data_at_node(succs[0])\n",
    "                sigma_F = data_at_node(succs[1])\n",
    "\n",
    "                #keep tabs on how training is going\n",
    "                # if node[0] > self.current_max_depth:\n",
    "                    # self.current_max_depth = node[0]\n",
    "                    # if self.current_max_depth % 30 == 0:\n",
    "                    #     if self.current_max_depth > 1:\n",
    "                    #         print('current max depth', self.current_max_depth)\n",
    "                # print('current max depth', self.current_max_depth)\n",
    "\n",
    "                # construct_subtree on the sucessors\n",
    "                construct_subtree(succs[0], sigma_T)\n",
    "                construct_subtree(succs[1], sigma_F)\n",
    "\n",
    "        # train the dt\n",
    "        construct_subtree((0,0), np.ones(n).astype(bool))\n",
    "        self.is_trained = True\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict classes for examples in X.\n",
    "        The predicted class of an input sample is the majority vote by the trees in the forest.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The test samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        preds : array of shape (n_samples,)\n",
    "            The predicted classes.\n",
    "\n",
    "        \"\"\"\n",
    "        # first check to see if the tree is empty/trained\n",
    "        if self.is_trained:\n",
    "\n",
    "            preds = np.zeros(len(X)).astype(np.int8)\n",
    "            for i in range(len(X)):\n",
    "                X_ = X[i]\n",
    "                a,b = 0,0\n",
    "                tnode = self.nodes[(a,b)]\n",
    "                while not tnode['is_leaf']:\n",
    "                    check = X_[tnode['j']] <= tnode['xi']\n",
    "\n",
    "                    if check:\n",
    "                        b = 2*b\n",
    "                    else:\n",
    "                        b = 2*b + 1\n",
    "\n",
    "                    a += 1\n",
    "                    tnode = self.nodes[(a,b)]\n",
    "\n",
    "                if tnode['is_leaf']:\n",
    "                    preds[i] = tnode['g']\n",
    "\n",
    "            return preds\n",
    "        else:\n",
    "            print('tree not finished training!')\n",
    "\n",
    "\n",
    "    def n_leaves(self):\n",
    "        \"\"\"\n",
    "        Get the number of leaf nodes in a tree (number of regions created in feature space).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        temp : int\n",
    "            Number of leaf nodes in the classifier.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.leaf_count\n",
    "\n",
    "    def get_depth(self):\n",
    "        \"\"\"\n",
    "        Return the depth of the decision tree. The depth of a tree is the maximum distance between the root and any leaf.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        max_depth : int\n",
    "            The maximum depth of the tree.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        max_depth = -1\n",
    "        for node in self.nodes.keys():\n",
    "            if node[0] > max_depth:\n",
    "                max_depth = node[0]\n",
    "        return max_depth\n",
    "\n",
    "    def feature_importances(self):\n",
    "        \"\"\"\n",
    "        Compute the risk reduction feature importances.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        impurities : array-like of shape (n_features,)\n",
    "            Risk reduction feature importances.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        impurities = np.zeros([self.d])\n",
    "        for node in self.nodes:\n",
    "            if self.nodes[node]['j'] is not None:\n",
    "                impurities[self.nodes[node]['j']] += self.nodes[node]['risk_reduction']\n",
    "\n",
    "        return impurities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "brazilian-amateur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in ./.local/lib/python3.9/site-packages (1.3.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blond-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PU ExtraTrees - A Random Forest Classifier for PU Learning\n",
    "# from tree import PUExtraTree\n",
    "from joblib import Parallel, delayed\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "class PUExtraTrees:\n",
    "    def __init__(self, n_estimators = 100,\n",
    "                 risk_estimator = 'nnPU',\n",
    "                 loss = 'quadratic',\n",
    "                 max_depth = None,\n",
    "                 min_samples_leaf = 1,\n",
    "                 max_features = 'sqrt',\n",
    "                 max_candidates = 1,\n",
    "                 n_jobs = 1):\n",
    "        \"\"\"\n",
    "        An extra-trees binary classifier that can be trained using only positive and unlabeled samples, or positive and negative samples.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        risk_estimator : {\"PN\", \"uPU\", \"nnPU\"}, default='nnPU'\n",
    "            PU data based risk estimator. Supports supervised (PN) learning, unbiased PU (uPU) learning and nonnegative PU (nnPU) learning.\n",
    "        loss : {\"quadratic\", \"logistic\"}, default='quadratic'\n",
    "            The function to measure the cost of making an incorrect prediction. Supported loss functions are:\n",
    "            \"quadratic\" l(v,y) = (1-vy)^2 and\n",
    "            \"logistic\" l(v,y) = ln(1+exp(-vy)).\n",
    "        max_depth : int or None, default=None\n",
    "            The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_leaf samples.\n",
    "        min_samples_leaf : int, default=1\n",
    "            The minimum number of samples required to be at a leaf node. The default is 1.\n",
    "        max_features : int or {\"sqrt\", \"all\"}, default=\"sqrt\"\n",
    "            The number of features to consider when looking for the best split. If \"sqrt\", then max_features = ceil(sqrt(n_features)). If \"all\", then max_features = n_features.\n",
    "        max_candidates : int, default=1\n",
    "            Number of randomly chosen split points to consider for each candidate feature.\n",
    "        n_jobs : int, default=1\n",
    "            The number of jobs to run in parallel. fit and predict are all parallelized over the trees.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_estimators = n_estimators\n",
    "        self.risk_estimator = risk_estimator\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.max_candidates = max_candidates\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "        self.leaf_count = 0\n",
    "        self.current_max_depth = 0\n",
    "        self.is_trained = False # indicate if tree empty/trained\n",
    "\n",
    "    def train_tree(self, P = None, U = None, N = None, pi = None):\n",
    "        \"\"\"\n",
    "        Train a single decision tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        P : array-like of shape (n_p, n_features), default=None\n",
    "            Training samples from the positive class.\n",
    "        U : array-like of shape (n_u, n_features), default=None\n",
    "            Unlabelled training samples.\n",
    "        N : array-like of shape (n_n, n_features), default=None\n",
    "            Training samples from the negative class if performing supervised (PN) learning.\n",
    "        pi : float\n",
    "            Prior probability that an example belongs to the positive class.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        g : ET classifier\n",
    "            An instance of the single tree RF classifier.\n",
    "\n",
    "        \"\"\"\n",
    "        g = PUExtraTree(risk_estimator = self.risk_estimator,\n",
    "                        loss = self.loss,\n",
    "                        max_depth = self.max_depth,\n",
    "                        min_samples_leaf = self.min_samples_leaf,\n",
    "                        max_features = self.max_features,\n",
    "                        max_candidates = self.max_candidates)\n",
    "        g.fit(P = P, U = U, N = N, pi = pi)\n",
    "        return g\n",
    "\n",
    "    def predict_tree(self, g, X):\n",
    "        \"\"\"\n",
    "        Predict classes for examples in X using the single DT g.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The test samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        preds : array of shape (n_samples,)\n",
    "            The predicted classes.\n",
    "\n",
    "        \"\"\"\n",
    "        return g.predict(X)\n",
    "\n",
    "    def fit(self, P = None, U = None, N = None, pi = None):\n",
    "        \"\"\"\n",
    "        Train the random forest.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pi : float\n",
    "            Prior probability that an example belongs to the positive class.\n",
    "        P : array-like of shape (n_p, n_features), default=None\n",
    "            Training samples from the positive class.\n",
    "        U : array-like of shape (n_u, n_features), default=None\n",
    "            Unlabeled training samples.\n",
    "        N : array-like of shape (n_n, n_features), default=None\n",
    "            Training samples from the negative class if performing PN learning.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "            Returns instance of self.\n",
    "\n",
    "        \"\"\"\n",
    "        self.gs = Parallel(n_jobs = min(self.n_jobs, self.n_estimators), prefer=\"threads\")(delayed(self.train_tree)(P = P, U = U, N = N, pi = pi) for i in range(self.n_estimators))\n",
    "        self.is_trained = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict classes for examples in X.\n",
    "        The predicted class of an input sample is the majority vote by the trees in the forest.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The test samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        preds : array of shape (n_samples,)\n",
    "            The predicted classes.\n",
    "\n",
    "        \"\"\"\n",
    "        self.preds = Parallel(n_jobs = min(self.n_jobs, self.n_estimators), prefer=\"threads\")(delayed(self.predict_tree)(g, X) for g in self.gs)\n",
    "        return scipy.stats.mode(np.array(self.preds), axis = 0)[0]\n",
    "\n",
    "    def n_leaves(self, tree):\n",
    "        \"\"\"\n",
    "        Get the number of leaf nodes in a specified tree\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tree : int\n",
    "            The index of the tree.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Number of leaf nodes in the specified tree.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return self.gs[tree].n_leaves()\n",
    "\n",
    "    def get_depth(self, tree):\n",
    "        \"\"\"\n",
    "        Get the depth of a specified tree in the forest.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tree : int\n",
    "            The index of the tree.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Depth of the specified tree.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return self.gs[tree].get_depth()\n",
    "\n",
    "    def get_max_depth(self):\n",
    "        \"\"\"\n",
    "        Return the depth of the deepest tree in the forest.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Maximum depth : int\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        depths = []\n",
    "        for tree in self.gs:\n",
    "            depths += [tree.get_depth()]\n",
    "        return np.max(depths)\n",
    "\n",
    "    def feature_importances(self):\n",
    "        \"\"\"\n",
    "        Get the risk reduction feature importances.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        importances : array of shape (n_features,)\n",
    "            The risk reduction feature importances.\n",
    "\n",
    "        \"\"\"\n",
    "        importances = np.zeros([self.gs[0].d])\n",
    "        for tree in self.gs:\n",
    "            importances += tree.feature_importances()/self.n_estimators\n",
    "\n",
    "        return importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "injured-class",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in ./.local/lib/python3.9/site-packages (1.2.3)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in ./.local/lib/python3.9/site-packages (from catboost) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.24 in /usr/lib/python3/dist-packages (from catboost) (1.1.5)\n",
      "Requirement already satisfied: plotly in ./.local/lib/python3.9/site-packages (from catboost) (5.20.0)\n",
      "Requirement already satisfied: graphviz in ./.local/lib/python3.9/site-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: scipy in /usr/lib/python3/dist-packages (from catboost) (1.6.0)\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (from catboost) (3.3.4)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in ./.local/lib/python3.9/site-packages (from plotly->catboost) (8.2.3)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from plotly->catboost) (20.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "multiple-halifax",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Bottleneck unit testing available.\n"
     ]
    }
   ],
   "source": [
    "from catboost.datasets import epsilon\n",
    "epsilon_train, epsilon_test = epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "weird-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(epsilon_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tested-consolidation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.010828</td>\n",
       "      <td>-0.019600</td>\n",
       "      <td>0.042215</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.014533</td>\n",
       "      <td>0.008864</td>\n",
       "      <td>-0.032517</td>\n",
       "      <td>0.011299</td>\n",
       "      <td>0.015190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036370</td>\n",
       "      <td>0.011610</td>\n",
       "      <td>0.010202</td>\n",
       "      <td>0.029232</td>\n",
       "      <td>-0.016093</td>\n",
       "      <td>0.018768</td>\n",
       "      <td>-0.010783</td>\n",
       "      <td>-0.003415</td>\n",
       "      <td>-0.013111</td>\n",
       "      <td>0.010197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.004703</td>\n",
       "      <td>0.016913</td>\n",
       "      <td>0.006024</td>\n",
       "      <td>-0.011034</td>\n",
       "      <td>0.035253</td>\n",
       "      <td>-0.021374</td>\n",
       "      <td>0.005930</td>\n",
       "      <td>-0.014309</td>\n",
       "      <td>0.039498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006867</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>-0.003425</td>\n",
       "      <td>-0.023347</td>\n",
       "      <td>-0.010011</td>\n",
       "      <td>-0.043020</td>\n",
       "      <td>-0.032248</td>\n",
       "      <td>-0.004244</td>\n",
       "      <td>-0.041520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002508</td>\n",
       "      <td>0.016845</td>\n",
       "      <td>-0.010293</td>\n",
       "      <td>-0.015634</td>\n",
       "      <td>0.018036</td>\n",
       "      <td>-0.023563</td>\n",
       "      <td>-0.005037</td>\n",
       "      <td>0.005102</td>\n",
       "      <td>0.037780</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000240</td>\n",
       "      <td>0.007758</td>\n",
       "      <td>0.016737</td>\n",
       "      <td>0.005199</td>\n",
       "      <td>-0.029202</td>\n",
       "      <td>-0.029026</td>\n",
       "      <td>-0.038255</td>\n",
       "      <td>-0.031005</td>\n",
       "      <td>-0.016839</td>\n",
       "      <td>-0.036780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.008525</td>\n",
       "      <td>-0.002561</td>\n",
       "      <td>0.024329</td>\n",
       "      <td>-0.005932</td>\n",
       "      <td>0.001965</td>\n",
       "      <td>-0.026056</td>\n",
       "      <td>-0.002887</td>\n",
       "      <td>-0.012315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006798</td>\n",
       "      <td>-0.017795</td>\n",
       "      <td>-0.039721</td>\n",
       "      <td>0.035222</td>\n",
       "      <td>0.012496</td>\n",
       "      <td>0.035623</td>\n",
       "      <td>-0.005441</td>\n",
       "      <td>0.018508</td>\n",
       "      <td>0.012023</td>\n",
       "      <td>-0.013321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.009352</td>\n",
       "      <td>0.001053</td>\n",
       "      <td>0.038326</td>\n",
       "      <td>0.006666</td>\n",
       "      <td>-0.005453</td>\n",
       "      <td>0.025037</td>\n",
       "      <td>-0.008886</td>\n",
       "      <td>-0.019346</td>\n",
       "      <td>-0.014963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010609</td>\n",
       "      <td>-0.008746</td>\n",
       "      <td>0.040222</td>\n",
       "      <td>-0.011202</td>\n",
       "      <td>0.057171</td>\n",
       "      <td>0.017244</td>\n",
       "      <td>0.015305</td>\n",
       "      <td>-0.019711</td>\n",
       "      <td>0.018008</td>\n",
       "      <td>-0.004310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.013020</td>\n",
       "      <td>0.030704</td>\n",
       "      <td>-0.007066</td>\n",
       "      <td>-0.001714</td>\n",
       "      <td>0.029920</td>\n",
       "      <td>-0.017332</td>\n",
       "      <td>0.019626</td>\n",
       "      <td>-0.016661</td>\n",
       "      <td>0.037513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.013792</td>\n",
       "      <td>0.008728</td>\n",
       "      <td>-0.003738</td>\n",
       "      <td>-0.000996</td>\n",
       "      <td>-0.010730</td>\n",
       "      <td>-0.036103</td>\n",
       "      <td>-0.014462</td>\n",
       "      <td>-0.019896</td>\n",
       "      <td>-0.044031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.013424</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.007270</td>\n",
       "      <td>0.015533</td>\n",
       "      <td>0.006022</td>\n",
       "      <td>-0.036704</td>\n",
       "      <td>0.014432</td>\n",
       "      <td>0.013070</td>\n",
       "      <td>0.007521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021567</td>\n",
       "      <td>-0.041164</td>\n",
       "      <td>-0.020147</td>\n",
       "      <td>0.010669</td>\n",
       "      <td>0.028408</td>\n",
       "      <td>0.031195</td>\n",
       "      <td>-0.002247</td>\n",
       "      <td>-0.022245</td>\n",
       "      <td>-0.041987</td>\n",
       "      <td>-0.020384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399997</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.043320</td>\n",
       "      <td>-0.000236</td>\n",
       "      <td>0.027009</td>\n",
       "      <td>0.009198</td>\n",
       "      <td>-0.008228</td>\n",
       "      <td>-0.034195</td>\n",
       "      <td>-0.010594</td>\n",
       "      <td>0.005818</td>\n",
       "      <td>0.013951</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000732</td>\n",
       "      <td>-0.025494</td>\n",
       "      <td>0.007514</td>\n",
       "      <td>0.030475</td>\n",
       "      <td>-0.015690</td>\n",
       "      <td>-0.017973</td>\n",
       "      <td>-0.016226</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>-0.017922</td>\n",
       "      <td>-0.029265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.011515</td>\n",
       "      <td>0.022920</td>\n",
       "      <td>-0.001483</td>\n",
       "      <td>0.007006</td>\n",
       "      <td>0.031677</td>\n",
       "      <td>-0.041371</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>0.042509</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005267</td>\n",
       "      <td>0.005969</td>\n",
       "      <td>0.014357</td>\n",
       "      <td>-0.021295</td>\n",
       "      <td>-0.010826</td>\n",
       "      <td>-0.024578</td>\n",
       "      <td>-0.039043</td>\n",
       "      <td>-0.035559</td>\n",
       "      <td>-0.012287</td>\n",
       "      <td>-0.034348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011778</td>\n",
       "      <td>0.018507</td>\n",
       "      <td>-0.019858</td>\n",
       "      <td>-0.027636</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>-0.009579</td>\n",
       "      <td>-0.009987</td>\n",
       "      <td>0.005285</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021829</td>\n",
       "      <td>0.038819</td>\n",
       "      <td>-0.023783</td>\n",
       "      <td>0.025310</td>\n",
       "      <td>-0.007459</td>\n",
       "      <td>-0.022358</td>\n",
       "      <td>-0.001420</td>\n",
       "      <td>0.020995</td>\n",
       "      <td>0.011790</td>\n",
       "      <td>-0.005094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows Ã— 2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6     \\\n",
       "0        1.0 -0.010828 -0.019600  0.042215  0.003000  0.014533  0.008864   \n",
       "1       -1.0 -0.004703  0.016913  0.006024 -0.011034  0.035253 -0.021374   \n",
       "2        1.0  0.002508  0.016845 -0.010293 -0.015634  0.018036 -0.023563   \n",
       "3       -1.0 -0.000007  0.008525 -0.002561  0.024329 -0.005932  0.001965   \n",
       "4       -1.0 -0.009352  0.001053  0.038326  0.006666 -0.005453  0.025037   \n",
       "...      ...       ...       ...       ...       ...       ...       ...   \n",
       "399995   1.0 -0.013020  0.030704 -0.007066 -0.001714  0.029920 -0.017332   \n",
       "399996  -1.0  0.013424  0.002060  0.007270  0.015533  0.006022 -0.036704   \n",
       "399997  -1.0 -0.043320 -0.000236  0.027009  0.009198 -0.008228 -0.034195   \n",
       "399998   1.0 -0.011515  0.022920 -0.001483  0.007006  0.031677 -0.041371   \n",
       "399999   1.0  0.011778  0.018507 -0.019858 -0.027636  0.001108 -0.009579   \n",
       "\n",
       "            7         8         9     ...      1991      1992      1993  \\\n",
       "0      -0.032517  0.011299  0.015190  ...  0.036370  0.011610  0.010202   \n",
       "1       0.005930 -0.014309  0.039498  ...  0.006867  0.002999  0.000582   \n",
       "2      -0.005037  0.005102  0.037780  ... -0.000240  0.007758  0.016737   \n",
       "3      -0.026056 -0.002887 -0.012315  ...  0.006798 -0.017795 -0.039721   \n",
       "4      -0.008886 -0.019346 -0.014963  ... -0.010609 -0.008746  0.040222   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "399995  0.019626 -0.016661  0.037513  ...  0.000722  0.013792  0.008728   \n",
       "399996  0.014432  0.013070  0.007521  ...  0.021567 -0.041164 -0.020147   \n",
       "399997 -0.010594  0.005818  0.013951  ... -0.000732 -0.025494  0.007514   \n",
       "399998  0.017700  0.001839  0.042509  ... -0.005267  0.005969  0.014357   \n",
       "399999 -0.009987  0.005285  0.002558  ... -0.021829  0.038819 -0.023783   \n",
       "\n",
       "            1994      1995      1996      1997      1998      1999      2000  \n",
       "0       0.029232 -0.016093  0.018768 -0.010783 -0.003415 -0.013111  0.010197  \n",
       "1      -0.003425 -0.023347 -0.010011 -0.043020 -0.032248 -0.004244 -0.041520  \n",
       "2       0.005199 -0.029202 -0.029026 -0.038255 -0.031005 -0.016839 -0.036780  \n",
       "3       0.035222  0.012496  0.035623 -0.005441  0.018508  0.012023 -0.013321  \n",
       "4      -0.011202  0.057171  0.017244  0.015305 -0.019711  0.018008 -0.004310  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "399995 -0.003738 -0.000996 -0.010730 -0.036103 -0.014462 -0.019896 -0.044031  \n",
       "399996  0.010669  0.028408  0.031195 -0.002247 -0.022245 -0.041987 -0.020384  \n",
       "399997  0.030475 -0.015690 -0.017973 -0.016226  0.002462 -0.017922 -0.029265  \n",
       "399998 -0.021295 -0.010826 -0.024578 -0.039043 -0.035559 -0.012287 -0.034348  \n",
       "399999  0.025310 -0.007459 -0.022358 -0.001420  0.020995  0.011790 -0.005094  \n",
       "\n",
       "[400000 rows x 2001 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "attempted-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = epsilon_train[0]\n",
    "X = epsilon_train.drop(columns = 0)\n",
    "n_p = 1000\n",
    "positive_indices = np.random.choice(np.where(y_train == 1)[0], size = n_p, replace = False)\n",
    "P = X.iloc[positive_indices]\n",
    "U = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "coupled-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = epsilon_test.drop(columns = 0)\n",
    "y_test = epsilon_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "private-cleveland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005439</td>\n",
       "      <td>0.013331</td>\n",
       "      <td>-0.002186</td>\n",
       "      <td>-0.014590</td>\n",
       "      <td>0.015632</td>\n",
       "      <td>-0.032606</td>\n",
       "      <td>-0.004455</td>\n",
       "      <td>0.013611</td>\n",
       "      <td>0.024088</td>\n",
       "      <td>0.010940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029023</td>\n",
       "      <td>0.028153</td>\n",
       "      <td>-0.001714</td>\n",
       "      <td>-0.048453</td>\n",
       "      <td>-0.030330</td>\n",
       "      <td>-0.006301</td>\n",
       "      <td>-0.022238</td>\n",
       "      <td>-0.009459</td>\n",
       "      <td>0.027544</td>\n",
       "      <td>-0.026216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.033820</td>\n",
       "      <td>-0.048836</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>-0.028718</td>\n",
       "      <td>0.013421</td>\n",
       "      <td>-0.006827</td>\n",
       "      <td>0.053082</td>\n",
       "      <td>-0.016931</td>\n",
       "      <td>0.049545</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016412</td>\n",
       "      <td>0.005543</td>\n",
       "      <td>-0.017588</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>0.037386</td>\n",
       "      <td>-0.002206</td>\n",
       "      <td>0.023466</td>\n",
       "      <td>0.023459</td>\n",
       "      <td>0.036497</td>\n",
       "      <td>0.033899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004597</td>\n",
       "      <td>-0.042784</td>\n",
       "      <td>-0.004416</td>\n",
       "      <td>-0.005693</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>-0.025873</td>\n",
       "      <td>0.031471</td>\n",
       "      <td>0.059522</td>\n",
       "      <td>0.003261</td>\n",
       "      <td>0.015846</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020841</td>\n",
       "      <td>-0.030902</td>\n",
       "      <td>0.005387</td>\n",
       "      <td>-0.017727</td>\n",
       "      <td>-0.011851</td>\n",
       "      <td>0.007834</td>\n",
       "      <td>-0.002806</td>\n",
       "      <td>-0.004059</td>\n",
       "      <td>0.024565</td>\n",
       "      <td>-0.001050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.009080</td>\n",
       "      <td>0.017605</td>\n",
       "      <td>-0.009870</td>\n",
       "      <td>0.007386</td>\n",
       "      <td>0.021338</td>\n",
       "      <td>-0.042682</td>\n",
       "      <td>-0.004471</td>\n",
       "      <td>0.035229</td>\n",
       "      <td>0.037935</td>\n",
       "      <td>-0.005612</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025915</td>\n",
       "      <td>0.005119</td>\n",
       "      <td>0.022496</td>\n",
       "      <td>-0.005170</td>\n",
       "      <td>-0.023424</td>\n",
       "      <td>-0.026319</td>\n",
       "      <td>-0.036478</td>\n",
       "      <td>-0.036575</td>\n",
       "      <td>-0.002760</td>\n",
       "      <td>-0.021496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.011689</td>\n",
       "      <td>-0.021413</td>\n",
       "      <td>0.012358</td>\n",
       "      <td>-0.012036</td>\n",
       "      <td>-0.009324</td>\n",
       "      <td>-0.023587</td>\n",
       "      <td>0.007309</td>\n",
       "      <td>0.034027</td>\n",
       "      <td>-0.020042</td>\n",
       "      <td>-0.050917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006731</td>\n",
       "      <td>0.011447</td>\n",
       "      <td>-0.015999</td>\n",
       "      <td>-0.016919</td>\n",
       "      <td>0.047534</td>\n",
       "      <td>-0.004458</td>\n",
       "      <td>0.013541</td>\n",
       "      <td>0.036077</td>\n",
       "      <td>-0.004313</td>\n",
       "      <td>0.008140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.004446</td>\n",
       "      <td>0.022291</td>\n",
       "      <td>0.020920</td>\n",
       "      <td>0.020427</td>\n",
       "      <td>0.012598</td>\n",
       "      <td>0.030112</td>\n",
       "      <td>0.011091</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>-0.020676</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024602</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>0.016945</td>\n",
       "      <td>0.007586</td>\n",
       "      <td>0.037605</td>\n",
       "      <td>0.028264</td>\n",
       "      <td>-0.001602</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>0.006102</td>\n",
       "      <td>-0.003627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>-0.011844</td>\n",
       "      <td>0.020148</td>\n",
       "      <td>-0.026866</td>\n",
       "      <td>0.003328</td>\n",
       "      <td>0.027158</td>\n",
       "      <td>0.005969</td>\n",
       "      <td>0.017370</td>\n",
       "      <td>-0.011501</td>\n",
       "      <td>0.029321</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026503</td>\n",
       "      <td>-0.025448</td>\n",
       "      <td>-0.037498</td>\n",
       "      <td>-0.027911</td>\n",
       "      <td>0.010793</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>-0.025452</td>\n",
       "      <td>-0.013585</td>\n",
       "      <td>-0.004808</td>\n",
       "      <td>-0.020536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>0.026138</td>\n",
       "      <td>0.014778</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>-0.026422</td>\n",
       "      <td>0.009453</td>\n",
       "      <td>-0.011432</td>\n",
       "      <td>0.017415</td>\n",
       "      <td>0.048577</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>0.007717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026015</td>\n",
       "      <td>-0.003136</td>\n",
       "      <td>0.020917</td>\n",
       "      <td>-0.011271</td>\n",
       "      <td>0.040341</td>\n",
       "      <td>-0.020855</td>\n",
       "      <td>-0.020545</td>\n",
       "      <td>-0.022856</td>\n",
       "      <td>0.049474</td>\n",
       "      <td>-0.010935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>-0.031459</td>\n",
       "      <td>-0.035492</td>\n",
       "      <td>0.003074</td>\n",
       "      <td>-0.030789</td>\n",
       "      <td>-0.013167</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.035517</td>\n",
       "      <td>0.007557</td>\n",
       "      <td>-0.012632</td>\n",
       "      <td>-0.023446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031911</td>\n",
       "      <td>0.028818</td>\n",
       "      <td>0.015955</td>\n",
       "      <td>-0.005749</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>0.018634</td>\n",
       "      <td>0.010508</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.010319</td>\n",
       "      <td>-0.001581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>-0.024013</td>\n",
       "      <td>0.033481</td>\n",
       "      <td>-0.018320</td>\n",
       "      <td>-0.011766</td>\n",
       "      <td>0.036155</td>\n",
       "      <td>-0.000876</td>\n",
       "      <td>0.005860</td>\n",
       "      <td>0.011863</td>\n",
       "      <td>0.029360</td>\n",
       "      <td>-0.015718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041014</td>\n",
       "      <td>-0.020519</td>\n",
       "      <td>-0.003613</td>\n",
       "      <td>-0.002157</td>\n",
       "      <td>-0.011022</td>\n",
       "      <td>-0.019959</td>\n",
       "      <td>-0.032119</td>\n",
       "      <td>-0.042401</td>\n",
       "      <td>0.005638</td>\n",
       "      <td>-0.009653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1         2         3         4         5         6         7     \\\n",
       "0      0.005439  0.013331 -0.002186 -0.014590  0.015632 -0.032606 -0.004455   \n",
       "1      0.001442  0.033820 -0.048836  0.000652 -0.028718  0.013421 -0.006827   \n",
       "2      0.004597 -0.042784 -0.004416 -0.005693  0.000731 -0.025873  0.031471   \n",
       "3     -0.009080  0.017605 -0.009870  0.007386  0.021338 -0.042682 -0.004471   \n",
       "4     -0.011689 -0.021413  0.012358 -0.012036 -0.009324 -0.023587  0.007309   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "99995  0.002185  0.004446  0.022291  0.020920  0.020427  0.012598  0.030112   \n",
       "99996 -0.011844  0.020148 -0.026866  0.003328  0.027158  0.005969  0.017370   \n",
       "99997  0.026138  0.014778  0.001464 -0.026422  0.009453 -0.011432  0.017415   \n",
       "99998 -0.031459 -0.035492  0.003074 -0.030789 -0.013167  0.002857  0.035517   \n",
       "99999 -0.024013  0.033481 -0.018320 -0.011766  0.036155 -0.000876  0.005860   \n",
       "\n",
       "           8         9         10    ...      1991      1992      1993  \\\n",
       "0      0.013611  0.024088  0.010940  ...  0.029023  0.028153 -0.001714   \n",
       "1      0.053082 -0.016931  0.049545  ... -0.016412  0.005543 -0.017588   \n",
       "2      0.059522  0.003261  0.015846  ... -0.020841 -0.030902  0.005387   \n",
       "3      0.035229  0.037935 -0.005612  ... -0.025915  0.005119  0.022496   \n",
       "4      0.034027 -0.020042 -0.050917  ...  0.006731  0.011447 -0.015999   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "99995  0.011091  0.001913 -0.020676  ... -0.024602  0.006985  0.016945   \n",
       "99996 -0.011501  0.029321  0.006667  ...  0.026503 -0.025448 -0.037498   \n",
       "99997  0.048577  0.025200  0.007717  ...  0.026015 -0.003136  0.020917   \n",
       "99998  0.007557 -0.012632 -0.023446  ...  0.031911  0.028818  0.015955   \n",
       "99999  0.011863  0.029360 -0.015718  ... -0.041014 -0.020519 -0.003613   \n",
       "\n",
       "           1994      1995      1996      1997      1998      1999      2000  \n",
       "0     -0.048453 -0.030330 -0.006301 -0.022238 -0.009459  0.027544 -0.026216  \n",
       "1      0.005169  0.037386 -0.002206  0.023466  0.023459  0.036497  0.033899  \n",
       "2     -0.017727 -0.011851  0.007834 -0.002806 -0.004059  0.024565 -0.001050  \n",
       "3     -0.005170 -0.023424 -0.026319 -0.036478 -0.036575 -0.002760 -0.021496  \n",
       "4     -0.016919  0.047534 -0.004458  0.013541  0.036077 -0.004313  0.008140  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "99995  0.007586  0.037605  0.028264 -0.001602  0.001870  0.006102 -0.003627  \n",
       "99996 -0.027911  0.010793 -0.004598 -0.025452 -0.013585 -0.004808 -0.020536  \n",
       "99997 -0.011271  0.040341 -0.020855 -0.020545 -0.022856  0.049474 -0.010935  \n",
       "99998 -0.005749  0.003205  0.018634  0.010508  0.001254  0.010319 -0.001581  \n",
       "99999 -0.002157 -0.011022 -0.019959 -0.032119 -0.042401  0.005638 -0.009653  \n",
       "\n",
       "[100000 rows x 2000 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unavailable-catch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  1.]\n",
      "[-1.  1.]\n",
      "(100000, 2000)\n",
      "(1000, 2000)\n",
      "(400000, 2000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.unique(y_train))\n",
    "print(np.unique(y_test))\n",
    "print(X_test.shape)\n",
    "print(P.shape)\n",
    "print(U.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "nervous-chorus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      -0.009143\n",
       "2       0.013158\n",
       "3       0.014550\n",
       "4      -0.036574\n",
       "5       0.029358\n",
       "          ...   \n",
       "1996   -0.011096\n",
       "1997   -0.031525\n",
       "1998   -0.028132\n",
       "1999   -0.007734\n",
       "2000   -0.037095\n",
       "Name: 8, Length: 2000, dtype: float32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "prime-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "intense-scratch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PUExtraTrees at 0x7f67f899b3a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = PUExtraTrees(n_estimators = 100,\n",
    "                 risk_estimator = 'nnPU',\n",
    "                 loss = 'quadratic',\n",
    "                 max_depth = None,\n",
    "                 min_samples_leaf = 1,\n",
    "                 max_features = 'sqrt',\n",
    "                 max_candidates = 1,\n",
    "                 n_jobs = 4)\n",
    "g.fit(P=P, U=U, pi = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "excited-burner",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3/dist-packages/pandas/core/indexes/base.py\", line 2898, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 70, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 101, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1032, in pandas._libs.hashtable.Int64HashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1039, in pandas._libs.hashtable.Int64HashTable.get_item\nKeyError: 0\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/23n0458/.local/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 273, in _wrap_func_call\n    return func()\n  File \"/home/23n0458/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 589, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/23n0458/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 589, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"<ipython-input-3-dac0d21905a4>\", line 102, in predict_tree\n    return g.predict(X)\n  File \"<ipython-input-1-c653a30f6348>\", line 413, in predict\n    X_ = X[i]\n  File \"/usr/lib/python3/dist-packages/pandas/core/frame.py\", line 2906, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/lib/python3/dist-packages/pandas/core/indexes/base.py\", line 2900, in get_loc\n    raise KeyError(key) from err\nKeyError: 0\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2b5a176c7cab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-dac0d21905a4>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \"\"\"\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"threads\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1697\u001b[0m             \u001b[0;31m# worker traceback.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1699\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_error_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1700\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1732\u001b[0m         \u001b[0;31m# called directly or if the generator is gc'ed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1733\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_job\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1734\u001b[0;31m             \u001b[0merror_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_warn_exit_early\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;31m# callback thread, and is stored internally. It's just waiting to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;31m# be returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_or_raise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For other backends, the main thread needs to run the retrieval step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_ERROR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "predictions = g.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = f1_score(predictions, test_labels, average= \"macro\")\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = PUExtraTrees(n_estimators = 100,\n",
    "                 risk_estimator = 'nnPU',\n",
    "                 loss = 'logistic',\n",
    "                 max_depth = None,\n",
    "                 min_samples_leaf = 1,\n",
    "                 max_features = 'sqrt',\n",
    "                 max_candidates = 1,\n",
    "                 n_jobs = 4)\n",
    "g1.fit(P=P, U=U, pi = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = g1.predict(X_test)\n",
    "print('Accuracy', (predictions1 == test_labels).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = PUExtraTrees(n_estimators = 100,\n",
    "                 risk_estimator = 'uPU',\n",
    "                 loss = 'logistic',\n",
    "                 max_depth = None,\n",
    "                 min_samples_leaf = 1,\n",
    "                 max_features = 'sqrt',\n",
    "                 max_candidates = 1,\n",
    "                 n_jobs = 4)\n",
    "g2.fit(P=P, U=U, pi = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = g2.predict(X_test)\n",
    "print('Accuracy', (predictions1 == test_labels).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-angola",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
