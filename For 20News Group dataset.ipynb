{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29962a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PU ExtraTree - A DT Classifier for PU Learning\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.sparse\n",
    "\n",
    "class PUExtraTree:\n",
    "    def __init__(self, risk_estimator = \"nnPU\",\n",
    "                 loss = \"quadratic\",\n",
    "                 max_depth = None,\n",
    "                 min_samples_leaf = 1,\n",
    "                 max_features = \"sqrt\",\n",
    "                 max_candidates = 1):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        risk_estimator : {\"PN\", \"uPU\", \"nnPU\"}, default='nnPU'\n",
    "            PU data based risk estimator. Supports supervised (PN) learning, unbiased PU (uPU) learning and nonnegative PU (nnPU) learning.\n",
    "        loss : {\"quadratic\", \"logistic\"}, default='quadratic'\n",
    "            The function to measure the cost of making an incorrect prediction. Supported loss functions are:\n",
    "            \"quadratic\" l(v,y) = (1-vy)^2 and\n",
    "            \"logistic\" l(v,y) = ln(1+exp(-vy)).\n",
    "        max_depth : int or None, default=None\n",
    "            The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_leaf samples.\n",
    "        min_samples_leaf : int, default=1\n",
    "            The minimum number of samples required to be at a leaf node. The default is 1.\n",
    "        max_features : int or {\"sqrt\", \"all\"}, default=\"sqrt\"\n",
    "            The number of features to consider when looking for the best split. If \"sqrt\", then max_features = ceil(sqrt(n_features)). If \"all\", then max_features = n_features.\n",
    "        max_candidates : int, default=1\n",
    "            Number of randomly chosen split points to consider for each candidate feature.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.risk_estimator = risk_estimator\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.max_candidates = max_candidates\n",
    "\n",
    "        self.is_trained = False # indicate if tree empty/trained\n",
    "        self.leaf_count = 0\n",
    "        self.current_max_depth = 0\n",
    "        self.nodes = {(0,0): {'data': None, 'j': None, 'xi': None, 'g': None,\n",
    "                              'is_leaf': None, 'risk_reduction': None}}\n",
    "\n",
    "\n",
    "    def create_successor(self, node, side):\n",
    "        \"\"\"\n",
    "        Create an empty child node (either T or F) in the tree.\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : tuple of length 2.\n",
    "            The parent node. First element is the depth in the tree, second element is the position at that depth.\n",
    "        side : {\"T\", \"F\"} or {\"L\", \"R\"}\n",
    "            Whether the node corresponds to a True or False split.\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        row, column = node\n",
    "        if side in ['T','L']:\n",
    "            self.nodes[(row+1, 2*column)] = {'data': None, 'j': None,\n",
    "                                                   'xi': None, 'g': None,\n",
    "                                                   'is_leaf': None, 'loss': None,\n",
    "                                                   'risk_reduction': None}\n",
    "        elif side in ['F','R']:\n",
    "            self.nodes[(row+1, 2*column+1)] = {'data': None, 'j': None,\n",
    "                                                     'xi': None, 'g': None,\n",
    "                                                     'is_leaf': None, 'loss': None,\n",
    "                                                     'risk_reduction': None}\n",
    "        elif side not in ['T','F','L','R']:\n",
    "            print('choose valid position of child node: \\'L\\', \\'R\\', \\'T\\', \\'F\\'')\n",
    "\n",
    "\n",
    "\n",
    "    def get_parent(self, node, return_truth_val):\n",
    "        \"\"\"\n",
    "        Return parent node, and optionally the relationship to child node (T/F).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : tuple of length 2\n",
    "            The child node.\n",
    "        return_truth_val : bool\n",
    "            Indicate whether the truth value should also be returned, that is, whether the child node corresponds to a true or false split.\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of length 2 or (tuple of length 2, bool)\n",
    "            The parent node, optionally the relationships to the parent nodes.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        parent = (node[0] - 1, node[1] // 2)\n",
    "        if return_truth_val:\n",
    "            if node[1] % 2 == 0:\n",
    "                return parent, True\n",
    "            else:\n",
    "                return parent, False\n",
    "        else:\n",
    "            return parent\n",
    "\n",
    "\n",
    "\n",
    "    def get_ancestory(self, node):\n",
    "        \"\"\"\n",
    "        Get parent nodes and relationship to the child nodes all the way to the root.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : tuple of length 2\n",
    "            Child node.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List of nodes.\n",
    "        bools : list\n",
    "            List of bools with the relationships to the parents.\n",
    "\n",
    "        \"\"\"\n",
    "        chain = [node]\n",
    "        bools = []\n",
    "        while chain[-1] != (0,0):\n",
    "            parent, relationship = self.get_parent(chain[-1], True)\n",
    "            chain += [parent]\n",
    "            bools += [relationship]\n",
    "\n",
    "        return chain[1:], bools\n",
    "\n",
    "\n",
    "    def load_tree(self, nodes):\n",
    "        \"\"\"\n",
    "        Load saved tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nodes : dictionary\n",
    "            Dictionary describing the trained decision tree. Typically output from self.nodes.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.nodes = nodes\n",
    "        self.is_trained = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, pi, P = None, U = None, N = None):\n",
    "        \"\"\"\n",
    "        Fit the decision tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pi : float\n",
    "            Prior probability that an example belongs to the positive class.\n",
    "        P : array-like of shape (n_p, n_features), default=None\n",
    "            Training samples from the positive class.\n",
    "        U : array-like of shape (n_u, n_features), default=None\n",
    "            Unlabeled training samples.\n",
    "        N : array-like of shape (n_n, n_features), default=None\n",
    "            Training samples from the negative class if performing PN learning.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "            Returns instance of self.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.risk_estimator in ['uPU', 'nnPU']:\n",
    "            X = np.concatenate((P, U), axis = 0)\n",
    "            y = np.concatenate((np.ones(len(P)), np.zeros(len(U))))\n",
    "        elif self.risk_estimator in ['PN']:\n",
    "            X = np.concatenate((P, N), axis = 0)\n",
    "            y = np.concatenate((np.ones(len(P)), -np.ones(len(N))))\n",
    "\n",
    "        # X = X.astype(np.float32)\n",
    "        y = y.astype(np.int8).flatten()\n",
    "        n, self.d = X.shape\n",
    "        n_p = (y == 1).sum()\n",
    "        n_u = (y == 0).sum()\n",
    "        n_n = (y == -1).sum()\n",
    "        self.pi = pi\n",
    "\n",
    "        if self.pi is None:\n",
    "            print('please specify pi')\n",
    "\n",
    "        if self.max_features == 'sqrt':\n",
    "            self.max_features = int(np.ceil(np.sqrt(X.shape[1])))\n",
    "        elif self.max_features == 'all':\n",
    "            self.max_features = X.shape[1]\n",
    "        elif self.max_features in [i for i in range(1, self.d+1)]:\n",
    "            None\n",
    "        else:\n",
    "            print('select valid number of max features to consider splitting on.')\n",
    "            return None\n",
    "\n",
    "\n",
    "        self.nodes[(0,0)]['data'] = scipy.sparse.coo_matrix(np.ones(n).astype(bool))\n",
    "\n",
    "        def data_at_node(node):\n",
    "            # return subset of training data in partition specified by certain node\n",
    "\n",
    "            if self.nodes[node]['data'] is not None:\n",
    "                return self.nodes[node]['data'].toarray()[0]\n",
    "            else:\n",
    "                # get indices of data at parent\n",
    "                parent_node, relationship = self.get_parent(node, True)\n",
    "                ind_parent = self.nodes[parent_node]['data'].toarray()[0].copy()\n",
    "                checks = (X[ind_parent, self.nodes[parent_node]['j']] <= self.nodes[parent_node]['xi']) == relationship\n",
    "                ind_parent[ind_parent] = checks.flatten()\n",
    "                self.nodes[node]['data'] = scipy.sparse.coo_matrix(ind_parent)\n",
    "                return ind_parent\n",
    "\n",
    "\n",
    "        def impurity_node(y_sigma):\n",
    "            # impurity of single node\n",
    "            if self.risk_estimator in [\"uPU\", \"nnPU\"]:\n",
    "                Wp = (y_sigma == 1).sum() * self.pi/n_p\n",
    "                Wn = (y_sigma == 0).sum()/n_u - Wp\n",
    "\n",
    "                if Wp + Wn == 0:\n",
    "                    vstar = float('inf')\n",
    "                else:\n",
    "                    vstar = Wp/(Wp + Wn)\n",
    "\n",
    "            elif self.risk_estimator in ['PN']:\n",
    "                Wp = (y_sigma == 1).sum() * self.pi/n_p\n",
    "                Wn = (y_sigma == -1).sum() * (1-self.pi)/n_n\n",
    "\n",
    "                if Wp + Wn == 0:\n",
    "                    vstar = float('inf')\n",
    "                else:\n",
    "                    vstar = Wp/(Wp + Wn)\n",
    "\n",
    "\n",
    "            if self.loss == \"quadratic\":\n",
    "                if self.risk_estimator == \"uPU\" and vstar == float('inf'):\n",
    "                    return -float('inf')\n",
    "                elif self.risk_estimator == \"nnPU\" and vstar > 1:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 4 * (Wp + Wn) * vstar * (1 - vstar)\n",
    "\n",
    "            elif self.loss == \"logistic\":\n",
    "                if self.risk_estimator == \"uPU\" and vstar > 1:\n",
    "                    return -float('inf')\n",
    "                elif self.risk_estimator in [\"uPU\", \"nnPU\", \"PN\"] and vstar in [0,1]:\n",
    "                    return 0\n",
    "                elif self.risk_estimator == \"nnPU\" and vstar > 1:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return (Wp + Wn) * (-vstar*np.log(vstar) - (1-vstar)*np.log(1-vstar))\n",
    "\n",
    "\n",
    "        def impurity_split(sigma, j, xi):\n",
    "            mask = (X[sigma, j] <= xi).flatten()\n",
    "            imT = impurity_node(y[sigma][mask])\n",
    "            imF = impurity_node(y[sigma][~mask])\n",
    "            return imT + imF\n",
    "\n",
    "\n",
    "        def regional_prediction_function(y_sigma):\n",
    "            if self.risk_estimator in [\"uPU\", \"nnPU\"]:\n",
    "                Wp = (y_sigma == 1).sum() * self.pi/n_p\n",
    "                Wn = (y_sigma == 0).sum()/n_u - Wp\n",
    "\n",
    "                if Wp + Wn == 0:\n",
    "                    vstar = float('inf')\n",
    "                else:\n",
    "                    vstar = Wp/(Wp + Wn)\n",
    "\n",
    "            elif self.risk_estimator in [\"PN\"]:\n",
    "                Wp = (y_sigma == 1).sum() * self.pi/n_p\n",
    "                Wn = (y_sigma == -1).sum() * (1-self.pi)/n_n\n",
    "\n",
    "                if Wp + Wn == 0:\n",
    "                    vstar = float('inf')\n",
    "                else:\n",
    "                    vstar = Wp/(Wp + Wn)\n",
    "\n",
    "            if vstar > 0.5:\n",
    "                return 1\n",
    "            elif vstar < 0.5:\n",
    "                return -1\n",
    "            elif vstar == 0.5:\n",
    "                return 2*np.random.binomial(1,0.5)-1\n",
    "\n",
    "\n",
    "        def construct_subtree(node, sigma):\n",
    "            # first check stopping criteria\n",
    "            impurity = impurity_node(y[sigma])\n",
    "\n",
    "            # check node pure\n",
    "            if self.risk_estimator in ['nnPU', 'PN']:\n",
    "                c1 = impurity > 0\n",
    "            elif self.risk_estimator == 'uPU':\n",
    "                if y[sigma].sum() == 0:\n",
    "                    c1 = impurity > 0\n",
    "                else:\n",
    "                    c1 = impurity > -float('inf')\n",
    "\n",
    "            # check max depth reached\n",
    "            if self.max_depth is None:\n",
    "                c2 = True\n",
    "            else:\n",
    "                c2 = node[0] < self.max_depth # max depth reached\n",
    "            c3 = self.min_samples_leaf < sigma.sum() # minimum samples in node reached\n",
    "            att_ptp = np.ptp(X[sigma], axis = 0)\n",
    "            c4 = att_ptp.sum() > 0 # check if there is any variability in features\n",
    "            # c4 = np.unique(X[sigma], axis = 0).shape[0] > 1\n",
    "            # check if any of the criteria satisfied\n",
    "            # if so, turn into a leaf node\n",
    "            if c1*c2*c3*c4 == 0:\n",
    "                self.nodes[node]['is_leaf'] = True\n",
    "                lab = regional_prediction_function(y[sigma])\n",
    "                self.nodes[node]['g'] = lab\n",
    "                self.nodes[node]['risk_reduction'] = 0\n",
    "                self.leaf_count += 1\n",
    "            else:\n",
    "                self.nodes[node]['is_leaf'] = False\n",
    "                # find valid nodes that can be used for a split\n",
    "                atts = []\n",
    "                for i in range(self.d):\n",
    "                    if att_ptp[i] > 0:\n",
    "                        atts += [i]\n",
    "\n",
    "                # ranomly pick candiates attributes\n",
    "                attributes = np.random.choice(atts, size = min(self.max_features, len(atts)), replace = False)\n",
    "                candidates = []\n",
    "                candidate_attributes = []\n",
    "                candidate_cut_points = []\n",
    "                for i in range(len(attributes)):\n",
    "                    for j in range(self.max_candidates):\n",
    "                        # need to guard against errors caused by finite precision\n",
    "                        a_,b_,c_,d_ = np.unique(X[sigma, attributes[i]])[[0,1,-2,-1]]\n",
    "                        cut_point = np.random.uniform(a_ + 2*(b_-a_)/5, c_ + 3*(d_-c_)/5)\n",
    "                        candidates += [[attributes[i], cut_point]]\n",
    "                        candidate_attributes += [attributes[i]]\n",
    "                        candidate_cut_points += [cut_point]\n",
    "\n",
    "                impurities = []\n",
    "                for i in range(len(candidates)):\n",
    "                    impurities += [impurity_split(sigma, candidate_attributes[i], candidate_cut_points[i])]\n",
    "\n",
    "                minimiser = np.argmin(impurities)\n",
    "                best_attribute = candidate_attributes[minimiser]\n",
    "                best_cut_point = candidate_cut_points[minimiser]\n",
    "\n",
    "                self.nodes[node]['j'] = int(best_attribute)\n",
    "                self.nodes[node]['xi'] = best_cut_point\n",
    "                self.nodes[node]['risk_reduction'] = impurity - impurities[minimiser]\n",
    "\n",
    "                # create successors of current node\n",
    "                self.create_successor(node, 'T')\n",
    "                self.create_successor(node, 'F')\n",
    "\n",
    "                # get set of data in these successors\n",
    "                succs = ((node[0]+1, 2*node[1]), (node[0]+1, 2*node[1]+1))\n",
    "                sigma_T = data_at_node(succs[0])\n",
    "                sigma_F = data_at_node(succs[1])\n",
    "\n",
    "                #keep tabs on how training is going\n",
    "                # if node[0] > self.current_max_depth:\n",
    "                    # self.current_max_depth = node[0]\n",
    "                    # if self.current_max_depth % 30 == 0:\n",
    "                    #     if self.current_max_depth > 1:\n",
    "                    #         print('current max depth', self.current_max_depth)\n",
    "                # print('current max depth', self.current_max_depth)\n",
    "\n",
    "                # construct_subtree on the sucessors\n",
    "                construct_subtree(succs[0], sigma_T)\n",
    "                construct_subtree(succs[1], sigma_F)\n",
    "\n",
    "        # train the dt\n",
    "        construct_subtree((0,0), np.ones(n).astype(bool))\n",
    "        self.is_trained = True\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict classes for examples in X.\n",
    "        The predicted class of an input sample is the majority vote by the trees in the forest.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The test samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        preds : array of shape (n_samples,)\n",
    "            The predicted classes.\n",
    "\n",
    "        \"\"\"\n",
    "        # first check to see if the tree is empty/trained\n",
    "        if self.is_trained:\n",
    "\n",
    "            preds = np.zeros(len(X)).astype(np.int8)\n",
    "            for i in range(len(X)):\n",
    "                X_ = X[i]\n",
    "                a,b = 0,0\n",
    "                tnode = self.nodes[(a,b)]\n",
    "                while not tnode['is_leaf']:\n",
    "                    check = X_[tnode['j']] <= tnode['xi']\n",
    "\n",
    "                    if check:\n",
    "                        b = 2*b\n",
    "                    else:\n",
    "                        b = 2*b + 1\n",
    "\n",
    "                    a += 1\n",
    "                    tnode = self.nodes[(a,b)]\n",
    "\n",
    "                if tnode['is_leaf']:\n",
    "                    preds[i] = tnode['g']\n",
    "\n",
    "            return preds\n",
    "        else:\n",
    "            print('tree not finished training!')\n",
    "\n",
    "\n",
    "    def n_leaves(self):\n",
    "        \"\"\"\n",
    "        Get the number of leaf nodes in a tree (number of regions created in feature space).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        temp : int\n",
    "            Number of leaf nodes in the classifier.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.leaf_count\n",
    "\n",
    "    def get_depth(self):\n",
    "        \"\"\"\n",
    "        Return the depth of the decision tree. The depth of a tree is the maximum distance between the root and any leaf.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        max_depth : int\n",
    "            The maximum depth of the tree.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        max_depth = -1\n",
    "        for node in self.nodes.keys():\n",
    "            if node[0] > max_depth:\n",
    "                max_depth = node[0]\n",
    "        return max_depth\n",
    "\n",
    "    def feature_importances(self):\n",
    "        \"\"\"\n",
    "        Compute the risk reduction feature importances.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        impurities : array-like of shape (n_features,)\n",
    "            Risk reduction feature importances.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        impurities = np.zeros([self.d])\n",
    "        for node in self.nodes:\n",
    "            if self.nodes[node]['j'] is not None:\n",
    "                impurities[self.nodes[node]['j']] += self.nodes[node]['risk_reduction']\n",
    "\n",
    "        return impurities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "maritime-essex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in ./.local/lib/python3.9/site-packages (1.3.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "peaceful-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PU ExtraTrees - A Random Forest Classifier for PU Learning\n",
    "# from tree import PUExtraTree\n",
    "from joblib import Parallel, delayed\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "class PUExtraTrees:\n",
    "    def __init__(self, n_estimators = 100,\n",
    "                 risk_estimator = 'nnPU',\n",
    "                 loss = 'quadratic',\n",
    "                 max_depth = None,\n",
    "                 min_samples_leaf = 1,\n",
    "                 max_features = 'sqrt',\n",
    "                 max_candidates = 1,\n",
    "                 n_jobs = 1):\n",
    "\n",
    "        self.n_estimators = n_estimators\n",
    "        self.risk_estimator = risk_estimator\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.max_candidates = max_candidates\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "        self.leaf_count = 0\n",
    "        self.current_max_depth = 0\n",
    "        self.is_trained = False # indicate if tree empty/trained\n",
    "\n",
    "    def train_tree(self, P = None, U = None, N = None, pi = None):\n",
    "\n",
    "        g = PUExtraTree(risk_estimator = self.risk_estimator,\n",
    "                        loss = self.loss,\n",
    "                        max_depth = self.max_depth,\n",
    "                        min_samples_leaf = self.min_samples_leaf,\n",
    "                        max_features = self.max_features,\n",
    "                        max_candidates = self.max_candidates)\n",
    "        g.fit(P = P, U = U, pi = pi)\n",
    "        return g\n",
    "\n",
    "    def predict_tree(self, g, X):\n",
    "\n",
    "        return g.predict(X)\n",
    "\n",
    "    def fit(self, P = None, U = None, N = None, pi = None):\n",
    "\n",
    "        self.gs = Parallel(n_jobs = min(self.n_jobs, self.n_estimators), prefer=\"threads\")(delayed(self.train_tree)(P = P, U = U, N = N, pi = pi) for i in range(self.n_estimators))\n",
    "        self.is_trained = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        self.preds = Parallel(n_jobs = min(self.n_jobs, self.n_estimators), prefer=\"threads\")(delayed(self.predict_tree)(g, X) for g in self.gs)\n",
    "        return scipy.stats.mode(np.array(self.preds), axis = 0)[0]\n",
    "\n",
    "    def n_leaves(self, tree):\n",
    "\n",
    "        return self.gs[tree].n_leaves()\n",
    "\n",
    "    def get_depth(self, tree):\n",
    "\n",
    "\n",
    "        return self.gs[tree].get_depth()\n",
    "\n",
    "    def get_max_depth(self):\n",
    "\n",
    "\n",
    "        depths = []\n",
    "        for tree in self.gs:\n",
    "            depths += [tree.get_depth()]\n",
    "        return np.max(depths)\n",
    "\n",
    "    def feature_importances(self):\n",
    "        importances = np.zeros([self.gs[0].d])\n",
    "        for tree in self.gs:\n",
    "            importances += tree.feature_importances()/self.n_estimators\n",
    "        return importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "expressed-clear",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.9/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.local/lib/python3.9/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in ./.local/lib/python3.9/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/lib/python3/dist-packages (from scikit-learn) (1.6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.9/site-packages (from scikit-learn) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "raised-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism',\n",
    " 'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove = (\"header\", \"footer\", \"quotes\"), categories = categories) \n",
    "\n",
    "categories1 =  ['sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    " 'talk.religion.misc']\n",
    "\n",
    "newsgroups_train1 = fetch_20newsgroups(subset='train', remove = (\"header\", \"footer\", \"quotes\"), categories = categories) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "environmental-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer =  TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "P = vectorizer.fit_transform(newsgroups_train.data)\n",
    "P = P.toarray()\n",
    "U = vectorizer.fit_transform(newsgroups_train1.data)\n",
    "U = U.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "remarkable-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = categories1 + categories\n",
    "news_groups2 = fetch_20newsgroups(subset='test', remove = (\"header\", \"footer\", \"quotes\"), categories = category) \n",
    "X_test = vectorizer.fit_transform(news_groups2.data)\n",
    "X_test = X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "southwest-collins",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PUExtraTrees at 0x7f4c1c8e2b80>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = PUExtraTrees(n_estimators = 100,\n",
    "                 risk_estimator = 'nnPU',\n",
    "                 loss = 'quadratic',\n",
    "                 max_depth = None,\n",
    "                 min_samples_leaf = 1,\n",
    "                 max_features = 'sqrt',\n",
    "                 max_candidates = 1,\n",
    "                 n_jobs = 4)\n",
    "g.fit(P=P, U=U, pi=0.56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "conservative-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "informative-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = news_groups2.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "smoking-durham",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.193433324432422\n"
     ]
    }
   ],
   "source": [
    "predictions = g.predict(X_test)\n",
    "print('Accuracy', (predictions == y_test ).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "distant-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predictions.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "supposed-tumor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09792837912363\n"
     ]
    }
   ],
   "source": [
    "f = f1_score(news_groups2.target, pred, average='macro')\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vocational-longer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PUExtraTrees at 0x7f4111773c70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_log = PUExtraTrees(n_estimators = 100,\n",
    "                 risk_estimator = 'uPU',\n",
    "                 loss = 'logistic',\n",
    "                 max_depth = None,\n",
    "                 min_samples_leaf = 1,\n",
    "                 max_features = 'sqrt',\n",
    "                 max_candidates = 1,\n",
    "                 n_jobs = 4)\n",
    "g_log.fit(P=P, U=U, pi=0.56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eight-therapy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.05164630908125332\n"
     ]
    }
   ],
   "source": [
    "predictions1 = g_log.predict(X_test)\n",
    "print('Accuracy', (predictions1 == news_groups2.target ).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "white-edgar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PUExtraTrees at 0x7f09b050cc40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1 = PUExtraTrees(n_estimators = 100,\n",
    "                 risk_estimator = 'nnPU',\n",
    "                 loss = 'logistic',\n",
    "                 max_depth = None,\n",
    "                 min_samples_leaf = 1,\n",
    "                 max_features = 'sqrt',\n",
    "                 max_candidates = 1,\n",
    "                 n_jobs = 4)\n",
    "g1.fit(P=P, U=U, pi=0.56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "shared-franklin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1936254693\n"
     ]
    }
   ],
   "source": [
    "ppredictions1 = g_log.predict(X_test)\n",
    "print('Accuracy', (predictions1 == news_groups2.target ).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "federal-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = predictions1.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "weird-detective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0979283793454\n"
     ]
    }
   ],
   "source": [
    "f = f1_score(news_groups2.target, pred1, average='macro')\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "august-audio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PUExtraTrees at 0x7f3f2db62af0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2 = PUExtraTrees(n_estimators = 100,\n",
    "                 risk_estimator = 'uPU',\n",
    "                 loss = 'quadratic',\n",
    "                 max_depth = None,\n",
    "                 min_samples_leaf = 1,\n",
    "                 max_features = 'sqrt',\n",
    "                 max_candidates = 1,\n",
    "                 n_jobs = 4)\n",
    "g2.fit(P=P, U=U, pi=0.56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "institutional-smooth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.05164630908125332\n"
     ]
    }
   ],
   "source": [
    "predictions2 = g2.predict(X_test)\n",
    "print('Accuracy', (predictions1 == news_groups2.target ).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "phantom-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = predictions2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "descending-bangladesh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004910996086352733"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = f1_score(news_groups2.target, pred2, average='macro')\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-honduras",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-holiday",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
