{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "controversial-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PU ExtraTree - A DT Classifier for PU Learning\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.sparse\n",
    "\n",
    "class PUExtraTree:\n",
    "    def __init__(self, risk_estimator = \"nnPU\",\n",
    "                 loss = \"quadratic\",\n",
    "                 max_depth = None,\n",
    "                 min_samples_leaf = 1,\n",
    "                 max_features = \"sqrt\",\n",
    "                 max_candidates = 1):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        risk_estimator : {\"PN\", \"uPU\", \"nnPU\"}, default='nnPU'\n",
    "            PU data based risk estimator. Supports supervised (PN) learning, unbiased PU (uPU) learning and nonnegative PU (nnPU) learning.\n",
    "        loss : {\"quadratic\", \"logistic\"}, default='quadratic'\n",
    "            The function to measure the cost of making an incorrect prediction. Supported loss functions are:\n",
    "            \"quadratic\" l(v,y) = (1-vy)^2 and\n",
    "            \"logistic\" l(v,y) = ln(1+exp(-vy)).\n",
    "        max_depth : int or None, default=None\n",
    "            The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_leaf samples.\n",
    "        min_samples_leaf : int, default=1\n",
    "            The minimum number of samples required to be at a leaf node. The default is 1.\n",
    "        max_features : int or {\"sqrt\", \"all\"}, default=\"sqrt\"\n",
    "            The number of features to consider when looking for the best split. If \"sqrt\", then max_features = ceil(sqrt(n_features)). If \"all\", then max_features = n_features.\n",
    "        max_candidates : int, default=1\n",
    "            Number of randomly chosen split points to consider for each candidate feature.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.risk_estimator = risk_estimator\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.max_candidates = max_candidates\n",
    "\n",
    "        self.is_trained = False # indicate if tree empty/trained\n",
    "        self.leaf_count = 0\n",
    "        self.current_max_depth = 0\n",
    "        self.nodes = {(0,0): {'data': None, 'j': None, 'xi': None, 'g': None,\n",
    "                              'is_leaf': None, 'risk_reduction': None}}\n",
    "\n",
    "\n",
    "    def create_successor(self, node, side):\n",
    "        \"\"\"\n",
    "        Create an empty child node (either T or F) in the tree.\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : tuple of length 2.\n",
    "            The parent node. First element is the depth in the tree, second element is the position at that depth.\n",
    "        side : {\"T\", \"F\"} or {\"L\", \"R\"}\n",
    "            Whether the node corresponds to a True or False split.\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        row, column = node\n",
    "        if side in ['T','L']:\n",
    "            self.nodes[(row+1, 2*column)] = {'data': None, 'j': None,\n",
    "                                                   'xi': None, 'g': None,\n",
    "                                                   'is_leaf': None, 'loss': None,\n",
    "                                                   'risk_reduction': None}\n",
    "        elif side in ['F','R']:\n",
    "            self.nodes[(row+1, 2*column+1)] = {'data': None, 'j': None,\n",
    "                                                     'xi': None, 'g': None,\n",
    "                                                     'is_leaf': None, 'loss': None,\n",
    "                                                     'risk_reduction': None}\n",
    "        elif side not in ['T','F','L','R']:\n",
    "            print('choose valid position of child node: \\'L\\', \\'R\\', \\'T\\', \\'F\\'')\n",
    "\n",
    "\n",
    "\n",
    "    def get_parent(self, node, return_truth_val):\n",
    "        \"\"\"\n",
    "        Return parent node, and optionally the relationship to child node (T/F).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : tuple of length 2\n",
    "            The child node.\n",
    "        return_truth_val : bool\n",
    "            Indicate whether the truth value should also be returned, that is, whether the child node corresponds to a true or false split.\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of length 2 or (tuple of length 2, bool)\n",
    "            The parent node, optionally the relationships to the parent nodes.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        parent = (node[0] - 1, node[1] // 2)\n",
    "        if return_truth_val:\n",
    "            if node[1] % 2 == 0:\n",
    "                return parent, True\n",
    "            else:\n",
    "                return parent, False\n",
    "        else:\n",
    "            return parent\n",
    "\n",
    "\n",
    "\n",
    "    def get_ancestory(self, node):\n",
    "        \"\"\"\n",
    "        Get parent nodes and relationship to the child nodes all the way to the root.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : tuple of length 2\n",
    "            Child node.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List of nodes.\n",
    "        bools : list\n",
    "            List of bools with the relationships to the parents.\n",
    "\n",
    "        \"\"\"\n",
    "        chain = [node]\n",
    "        bools = []\n",
    "        while chain[-1] != (0,0):\n",
    "            parent, relationship = self.get_parent(chain[-1], True)\n",
    "            chain += [parent]\n",
    "            bools += [relationship]\n",
    "\n",
    "        return chain[1:], bools\n",
    "\n",
    "\n",
    "    def load_tree(self, nodes):\n",
    "        \"\"\"\n",
    "        Load saved tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nodes : dictionary\n",
    "            Dictionary describing the trained decision tree. Typically output from self.nodes.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.nodes = nodes\n",
    "        self.is_trained = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, pi, P = None, U = None, N = None):\n",
    "        \"\"\"\n",
    "        Fit the decision tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pi : float\n",
    "            Prior probability that an example belongs to the positive class.\n",
    "        P : array-like of shape (n_p, n_features), default=None\n",
    "            Training samples from the positive class.\n",
    "        U : array-like of shape (n_u, n_features), default=None\n",
    "            Unlabeled training samples.\n",
    "        N : array-like of shape (n_n, n_features), default=None\n",
    "            Training samples from the negative class if performing PN learning.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "            Returns instance of self.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.risk_estimator in ['uPU', 'nnPU']:\n",
    "            X = np.concatenate((P, U), axis = 0)\n",
    "            y = np.concatenate((np.ones(len(P)), np.zeros(len(U))))\n",
    "        elif self.risk_estimator in ['PN']:\n",
    "            X = np.concatenate((P, N), axis = 0)\n",
    "            y = np.concatenate((np.ones(len(P)), -np.ones(len(N))))\n",
    "\n",
    "        # X = X.astype(np.float32)\n",
    "        y = y.astype(np.int8).flatten()\n",
    "        n, self.d = X.shape\n",
    "        n_p = (y == 1).sum()\n",
    "        n_u = (y == 0).sum()\n",
    "        n_n = (y == -1).sum()\n",
    "        self.pi = pi\n",
    "\n",
    "        if self.pi is None:\n",
    "            print('please specify pi')\n",
    "\n",
    "        if self.max_features == 'sqrt':\n",
    "            self.max_features = int(np.ceil(np.sqrt(X.shape[1])))\n",
    "        elif self.max_features == 'all':\n",
    "            self.max_features = X.shape[1]\n",
    "        elif self.max_features in [i for i in range(1, self.d+1)]:\n",
    "            None\n",
    "        else:\n",
    "            print('select valid number of max features to consider splitting on.')\n",
    "            return None\n",
    "\n",
    "\n",
    "        self.nodes[(0,0)]['data'] = scipy.sparse.coo_matrix(np.ones(n).astype(bool))\n",
    "\n",
    "        def data_at_node(node):\n",
    "            # return subset of training data in partition specified by certain node\n",
    "\n",
    "            if self.nodes[node]['data'] is not None:\n",
    "                return self.nodes[node]['data'].toarray()[0]\n",
    "            else:\n",
    "                # get indices of data at parent\n",
    "                parent_node, relationship = self.get_parent(node, True)\n",
    "                ind_parent = self.nodes[parent_node]['data'].toarray()[0].copy()\n",
    "                checks = (X[ind_parent, self.nodes[parent_node]['j']] <= self.nodes[parent_node]['xi']) == relationship\n",
    "                ind_parent[ind_parent] = checks.flatten()\n",
    "                self.nodes[node]['data'] = scipy.sparse.coo_matrix(ind_parent)\n",
    "                return ind_parent\n",
    "\n",
    "\n",
    "        def impurity_node(y_sigma):\n",
    "            # impurity of single node\n",
    "            if self.risk_estimator in [\"uPU\", \"nnPU\"]:\n",
    "                Wp = (y_sigma == 1).sum() * self.pi/n_p\n",
    "                Wn = (y_sigma == 0).sum()/n_u - Wp\n",
    "\n",
    "                if Wp + Wn == 0:\n",
    "                    vstar = float('inf')\n",
    "                else:\n",
    "                    vstar = Wp/(Wp + Wn)\n",
    "\n",
    "            elif self.risk_estimator in ['PN']:\n",
    "                Wp = (y_sigma == 1).sum() * self.pi/n_p\n",
    "                Wn = (y_sigma == -1).sum() * (1-self.pi)/n_n\n",
    "\n",
    "                if Wp + Wn == 0:\n",
    "                    vstar = float('inf')\n",
    "                else:\n",
    "                    vstar = Wp/(Wp + Wn)\n",
    "\n",
    "\n",
    "            if self.loss == \"quadratic\":\n",
    "                if self.risk_estimator == \"uPU\" and vstar == float('inf'):\n",
    "                    return -float('inf')\n",
    "                elif self.risk_estimator == \"nnPU\" and vstar > 1:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 4 * (Wp + Wn) * vstar * (1 - vstar)\n",
    "\n",
    "            elif self.loss == \"logistic\":\n",
    "                if self.risk_estimator == \"uPU\" and vstar > 1:\n",
    "                    return -float('inf')\n",
    "                elif self.risk_estimator in [\"uPU\", \"nnPU\", \"PN\"] and vstar in [0,1]:\n",
    "                    return 0\n",
    "                elif self.risk_estimator == \"nnPU\" and vstar > 1:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return (Wp + Wn) * (-vstar*np.log(vstar) - (1-vstar)*np.log(1-vstar))\n",
    "\n",
    "\n",
    "        def impurity_split(sigma, j, xi):\n",
    "            mask = (X[sigma, j] <= xi).flatten()\n",
    "            imT = impurity_node(y[sigma][mask])\n",
    "            imF = impurity_node(y[sigma][~mask])\n",
    "            return imT + imF\n",
    "\n",
    "\n",
    "        def regional_prediction_function(y_sigma):\n",
    "            if self.risk_estimator in [\"uPU\", \"nnPU\"]:\n",
    "                Wp = (y_sigma == 1).sum() * self.pi/n_p\n",
    "                Wn = (y_sigma == 0).sum()/n_u - Wp\n",
    "\n",
    "                if Wp + Wn == 0:\n",
    "                    vstar = float('inf')\n",
    "                else:\n",
    "                    vstar = Wp/(Wp + Wn)\n",
    "\n",
    "            elif self.risk_estimator in [\"PN\"]:\n",
    "                Wp = (y_sigma == 1).sum() * self.pi/n_p\n",
    "                Wn = (y_sigma == -1).sum() * (1-self.pi)/n_n\n",
    "\n",
    "                if Wp + Wn == 0:\n",
    "                    vstar = float('inf')\n",
    "                else:\n",
    "                    vstar = Wp/(Wp + Wn)\n",
    "\n",
    "            if vstar > 0.5:\n",
    "                return 1\n",
    "            elif vstar < 0.5:\n",
    "                return -1\n",
    "            elif vstar == 0.5:\n",
    "                return 2*np.random.binomial(1,0.5)-1\n",
    "\n",
    "\n",
    "        def construct_subtree(node, sigma):\n",
    "            # first check stopping criteria\n",
    "            impurity = impurity_node(y[sigma])\n",
    "\n",
    "            # check node pure\n",
    "            if self.risk_estimator in ['nnPU', 'PN']:\n",
    "                c1 = impurity > 0\n",
    "            elif self.risk_estimator == 'uPU':\n",
    "                if y[sigma].sum() == 0:\n",
    "                    c1 = impurity > 0\n",
    "                else:\n",
    "                    c1 = impurity > -float('inf')\n",
    "\n",
    "            # check max depth reached\n",
    "            if self.max_depth is None:\n",
    "                c2 = True\n",
    "            else:\n",
    "                c2 = node[0] < self.max_depth # max depth reached\n",
    "            c3 = self.min_samples_leaf < sigma.sum() # minimum samples in node reached\n",
    "            att_ptp = np.ptp(X[sigma], axis = 0)\n",
    "            c4 = att_ptp.sum() > 0 # check if there is any variability in features\n",
    "            # c4 = np.unique(X[sigma], axis = 0).shape[0] > 1\n",
    "            # check if any of the criteria satisfied\n",
    "            # if so, turn into a leaf node\n",
    "            if c1*c2*c3*c4 == 0:\n",
    "                self.nodes[node]['is_leaf'] = True\n",
    "                lab = regional_prediction_function(y[sigma])\n",
    "                self.nodes[node]['g'] = lab\n",
    "                self.nodes[node]['risk_reduction'] = 0\n",
    "                self.leaf_count += 1\n",
    "            else:\n",
    "                self.nodes[node]['is_leaf'] = False\n",
    "                # find valid nodes that can be used for a split\n",
    "                atts = []\n",
    "                for i in range(self.d):\n",
    "                    if att_ptp[i] > 0:\n",
    "                        atts += [i]\n",
    "\n",
    "                # ranomly pick candiates attributes\n",
    "                attributes = np.random.choice(atts, size = min(self.max_features, len(atts)), replace = False)\n",
    "                candidates = []\n",
    "                candidate_attributes = []\n",
    "                candidate_cut_points = []\n",
    "                for i in range(len(attributes)):\n",
    "                    for j in range(self.max_candidates):\n",
    "                        # need to guard against errors caused by finite precision\n",
    "                        a_,b_,c_,d_ = np.unique(X[sigma, attributes[i]])[[0,1,-2,-1]]\n",
    "                        cut_point = np.random.uniform(a_ + 2*(b_-a_)/5, c_ + 3*(d_-c_)/5)\n",
    "                        candidates += [[attributes[i], cut_point]]\n",
    "                        candidate_attributes += [attributes[i]]\n",
    "                        candidate_cut_points += [cut_point]\n",
    "\n",
    "                impurities = []\n",
    "                for i in range(len(candidates)):\n",
    "                    impurities += [impurity_split(sigma, candidate_attributes[i], candidate_cut_points[i])]\n",
    "\n",
    "                minimiser = np.argmin(impurities)\n",
    "                best_attribute = candidate_attributes[minimiser]\n",
    "                best_cut_point = candidate_cut_points[minimiser]\n",
    "\n",
    "                self.nodes[node]['j'] = int(best_attribute)\n",
    "                self.nodes[node]['xi'] = best_cut_point\n",
    "                self.nodes[node]['risk_reduction'] = impurity - impurities[minimiser]\n",
    "\n",
    "                # create successors of current node\n",
    "                self.create_successor(node, 'T')\n",
    "                self.create_successor(node, 'F')\n",
    "\n",
    "                # get set of data in these successors\n",
    "                succs = ((node[0]+1, 2*node[1]), (node[0]+1, 2*node[1]+1))\n",
    "                sigma_T = data_at_node(succs[0])\n",
    "                sigma_F = data_at_node(succs[1])\n",
    "\n",
    "                #keep tabs on how training is going\n",
    "                # if node[0] > self.current_max_depth:\n",
    "                    # self.current_max_depth = node[0]\n",
    "                    # if self.current_max_depth % 30 == 0:\n",
    "                    #     if self.current_max_depth > 1:\n",
    "                    #         print('current max depth', self.current_max_depth)\n",
    "                # print('current max depth', self.current_max_depth)\n",
    "\n",
    "                # construct_subtree on the sucessors\n",
    "                construct_subtree(succs[0], sigma_T)\n",
    "                construct_subtree(succs[1], sigma_F)\n",
    "\n",
    "        # train the dt\n",
    "        construct_subtree((0,0), np.ones(n).astype(bool))\n",
    "        self.is_trained = True\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict classes for examples in X.\n",
    "        The predicted class of an input sample is the majority vote by the trees in the forest.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The test samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        preds : array of shape (n_samples,)\n",
    "            The predicted classes.\n",
    "\n",
    "        \"\"\"\n",
    "        # first check to see if the tree is empty/trained\n",
    "        if self.is_trained:\n",
    "\n",
    "            preds = np.zeros(len(X)).astype(np.int8)\n",
    "            for i in range(len(X)):\n",
    "                X_ = X[i]\n",
    "                a,b = 0,0\n",
    "                tnode = self.nodes[(a,b)]\n",
    "                while not tnode['is_leaf']:\n",
    "                    check = X_[tnode['j']] <= tnode['xi']\n",
    "\n",
    "                    if check:\n",
    "                        b = 2*b\n",
    "                    else:\n",
    "                        b = 2*b + 1\n",
    "\n",
    "                    a += 1\n",
    "                    tnode = self.nodes[(a,b)]\n",
    "\n",
    "                if tnode['is_leaf']:\n",
    "                    preds[i] = tnode['g']\n",
    "\n",
    "            return preds\n",
    "        else:\n",
    "            print('tree not finished training!')\n",
    "\n",
    "\n",
    "    def n_leaves(self):\n",
    "        \"\"\"\n",
    "        Get the number of leaf nodes in a tree (number of regions created in feature space).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        temp : int\n",
    "            Number of leaf nodes in the classifier.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.leaf_count\n",
    "\n",
    "    def get_depth(self):\n",
    "        \"\"\"\n",
    "        Return the depth of the decision tree. The depth of a tree is the maximum distance between the root and any leaf.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        max_depth : int\n",
    "            The maximum depth of the tree.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        max_depth = -1\n",
    "        for node in self.nodes.keys():\n",
    "            if node[0] > max_depth:\n",
    "                max_depth = node[0]\n",
    "        return max_depth\n",
    "\n",
    "    def feature_importances(self):\n",
    "        \"\"\"\n",
    "        Compute the risk reduction feature importances.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        impurities : array-like of shape (n_features,)\n",
    "            Risk reduction feature importances.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        impurities = np.zeros([self.d])\n",
    "        for node in self.nodes:\n",
    "            if self.nodes[node]['j'] is not None:\n",
    "                impurities[self.nodes[node]['j']] += self.nodes[node]['risk_reduction']\n",
    "\n",
    "        return impurities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "architectural-poker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in ./.local/lib/python3.9/site-packages (1.3.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "automotive-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PU ExtraTrees - A Random Forest Classifier for PU Learning\n",
    "# from tree import PUExtraTree\n",
    "from joblib import Parallel, delayed\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "class PUExtraTrees:\n",
    "    def __init__(self, n_estimators = 100,\n",
    "                 risk_estimator = 'nnPU',\n",
    "                 loss = 'quadratic',\n",
    "                 max_depth = None,\n",
    "                 min_samples_leaf = 1,\n",
    "                 max_features = 'sqrt',\n",
    "                 max_candidates = 1,\n",
    "                 n_jobs = 1):\n",
    "        \"\"\"\n",
    "        An extra-trees binary classifier that can be trained using only positive and unlabeled samples, or positive and negative samples.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        risk_estimator : {\"PN\", \"uPU\", \"nnPU\"}, default='nnPU'\n",
    "            PU data based risk estimator. Supports supervised (PN) learning, unbiased PU (uPU) learning and nonnegative PU (nnPU) learning.\n",
    "        loss : {\"quadratic\", \"logistic\"}, default='quadratic'\n",
    "            The function to measure the cost of making an incorrect prediction. Supported loss functions are:\n",
    "            \"quadratic\" l(v,y) = (1-vy)^2 and\n",
    "            \"logistic\" l(v,y) = ln(1+exp(-vy)).\n",
    "        max_depth : int or None, default=None\n",
    "            The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_leaf samples.\n",
    "        min_samples_leaf : int, default=1\n",
    "            The minimum number of samples required to be at a leaf node. The default is 1.\n",
    "        max_features : int or {\"sqrt\", \"all\"}, default=\"sqrt\"\n",
    "            The number of features to consider when looking for the best split. If \"sqrt\", then max_features = ceil(sqrt(n_features)). If \"all\", then max_features = n_features.\n",
    "        max_candidates : int, default=1\n",
    "            Number of randomly chosen split points to consider for each candidate feature.\n",
    "        n_jobs : int, default=1\n",
    "            The number of jobs to run in parallel. fit and predict are all parallelized over the trees.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_estimators = n_estimators\n",
    "        self.risk_estimator = risk_estimator\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.max_candidates = max_candidates\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "        self.leaf_count = 0\n",
    "        self.current_max_depth = 0\n",
    "        self.is_trained = False # indicate if tree empty/trained\n",
    "\n",
    "    def train_tree(self, P = None, U = None, N = None, pi = None):\n",
    "        \"\"\"\n",
    "        Train a single decision tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        P : array-like of shape (n_p, n_features), default=None\n",
    "            Training samples from the positive class.\n",
    "        U : array-like of shape (n_u, n_features), default=None\n",
    "            Unlabelled training samples.\n",
    "        N : array-like of shape (n_n, n_features), default=None\n",
    "            Training samples from the negative class if performing supervised (PN) learning.\n",
    "        pi : float\n",
    "            Prior probability that an example belongs to the positive class.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        g : ET classifier\n",
    "            An instance of the single tree RF classifier.\n",
    "\n",
    "        \"\"\"\n",
    "        g = PUExtraTree(risk_estimator = self.risk_estimator,\n",
    "                        loss = self.loss,\n",
    "                        max_depth = self.max_depth,\n",
    "                        min_samples_leaf = self.min_samples_leaf,\n",
    "                        max_features = self.max_features,\n",
    "                        max_candidates = self.max_candidates)\n",
    "        g.fit(P = P, U = U, N = N, pi = pi)\n",
    "        return g\n",
    "\n",
    "    def predict_tree(self, g, X):\n",
    "        \"\"\"\n",
    "        Predict classes for examples in X using the single DT g.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The test samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        preds : array of shape (n_samples,)\n",
    "            The predicted classes.\n",
    "\n",
    "        \"\"\"\n",
    "        return g.predict(X)\n",
    "\n",
    "    def fit(self, P = None, U = None, N = None, pi = None):\n",
    "        \"\"\"\n",
    "        Train the random forest.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pi : float\n",
    "            Prior probability that an example belongs to the positive class.\n",
    "        P : array-like of shape (n_p, n_features), default=None\n",
    "            Training samples from the positive class.\n",
    "        U : array-like of shape (n_u, n_features), default=None\n",
    "            Unlabeled training samples.\n",
    "        N : array-like of shape (n_n, n_features), default=None\n",
    "            Training samples from the negative class if performing PN learning.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "            Returns instance of self.\n",
    "\n",
    "        \"\"\"\n",
    "        self.gs = Parallel(n_jobs = min(self.n_jobs, self.n_estimators), prefer=\"threads\")(delayed(self.train_tree)(P = P, U = U, N = N, pi = pi) for i in range(self.n_estimators))\n",
    "        self.is_trained = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict classes for examples in X.\n",
    "        The predicted class of an input sample is the majority vote by the trees in the forest.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The test samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        preds : array of shape (n_samples,)\n",
    "            The predicted classes.\n",
    "\n",
    "        \"\"\"\n",
    "        self.preds = Parallel(n_jobs = min(self.n_jobs, self.n_estimators), prefer=\"threads\")(delayed(self.predict_tree)(g, X) for g in self.gs)\n",
    "        return scipy.stats.mode(np.array(self.preds), axis = 0)[0]\n",
    "\n",
    "    def n_leaves(self, tree):\n",
    "        \"\"\"\n",
    "        Get the number of leaf nodes in a specified tree\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tree : int\n",
    "            The index of the tree.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Number of leaf nodes in the specified tree.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return self.gs[tree].n_leaves()\n",
    "\n",
    "    def get_depth(self, tree):\n",
    "        \"\"\"\n",
    "        Get the depth of a specified tree in the forest.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tree : int\n",
    "            The index of the tree.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Depth of the specified tree.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return self.gs[tree].get_depth()\n",
    "\n",
    "    def get_max_depth(self):\n",
    "        \"\"\"\n",
    "        Return the depth of the deepest tree in the forest.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Maximum depth : int\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        depths = []\n",
    "        for tree in self.gs:\n",
    "            depths += [tree.get_depth()]\n",
    "        return np.max(depths)\n",
    "\n",
    "    def feature_importances(self):\n",
    "        \"\"\"\n",
    "        Get the risk reduction feature importances.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        importances : array of shape (n_features,)\n",
    "            The risk reduction feature importances.\n",
    "\n",
    "        \"\"\"\n",
    "        importances = np.zeros([self.gs[0].d])\n",
    "        for tree in self.gs:\n",
    "            importances += tree.feature_importances()/self.n_estimators\n",
    "\n",
    "        return importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "round-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "judicial-berlin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9397\n",
      "Number of leaves in 3rd tree of forest: 263\n",
      "Maximum depth of any tree in forest: 25\n",
      "Depth of the 3rd tree in forest 17\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "# from trees import PUExtraTrees\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# fetch mnist digits\n",
    "X, y = fetch_openml('mnist_784', return_X_y = True, as_frame = False)\n",
    "y = y.astype(np.int8)\n",
    "\n",
    "# convert to binary labels\n",
    "y[y % 2 == 1] = -1 # Odd digits form N class\n",
    "y[y % 2 == 0] = 1 # Even digits form P class\n",
    "\n",
    "pi = (y == 1).mean()\n",
    "X_train, y_train, X_test, y_test = X[:60000], y[:60000], X[60000:], y[60000:]\n",
    "\n",
    "# construct P and U sets for training\n",
    "n_p = 1000\n",
    "positive_indices = np.random.choice(np.where(y_train == 1)[0], size = n_p, replace = False)\n",
    "P = X_train[positive_indices]\n",
    "U = X_train.copy()\n",
    "\n",
    "g = PUExtraTrees(n_estimators = 100,\n",
    "                 risk_estimator = 'nnPU',\n",
    "                 loss = 'quadratic',\n",
    "                 max_depth = None,\n",
    "                 min_samples_leaf = 1,\n",
    "                 max_features = 'sqrt',\n",
    "                 max_candidates = 1,\n",
    "                 n_jobs = 4)\n",
    "\n",
    "g.fit(P=P, U=U, pi=pi)\n",
    "predictions = g.predict(X_test)\n",
    "\n",
    "print('Accuracy', (predictions == y_test).mean())\n",
    "\n",
    "print('Number of leaves in 3rd tree of forest:', g.n_leaves(3-1))\n",
    "print('Maximum depth of any tree in forest:', g.get_max_depth())\n",
    "print('Depth of the 3rd tree in forest', g.get_depth(3-1))\n",
    "\n",
    "importances = g.feature_importances()\n",
    "plt.figure()\n",
    "plt.imshow(importances.reshape(28,28), cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "vocal-chess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.938\n"
     ]
    }
   ],
   "source": [
    "g1 = PUExtraTrees(n_estimators = 100,\n",
    "                 risk_estimator = 'nnPU',\n",
    "                 loss = 'logistic',\n",
    "                 max_depth = None,\n",
    "                 min_samples_leaf = 1,\n",
    "                 max_features = 'sqrt',\n",
    "                 max_candidates = 1,\n",
    "                 n_jobs = 4)\n",
    "g1.fit(P=P, U=U, pi = pi)\n",
    "predictions = g1.predict(X_test)\n",
    "print('Accuracy', (predictions == y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "taken-wages",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9034\n"
     ]
    }
   ],
   "source": [
    "g2 = PUExtraTrees(n_estimators = 100,\n",
    "                 risk_estimator = 'uPU',\n",
    "                 loss = 'logistic',\n",
    "                 max_depth = None,\n",
    "                 min_samples_leaf = 1,\n",
    "                 max_features = 'sqrt',\n",
    "                 max_candidates = 1,\n",
    "                 n_jobs = 4)\n",
    "g2.fit(P=P, U=U, pi = pi)\n",
    "predictions1 = g2.predict(X_test)\n",
    "print('Accuracy', (predictions1 == y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cosmetic-luther",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.5074\n"
     ]
    }
   ],
   "source": [
    "g3 = PUExtraTrees(n_estimators = 100,\n",
    "                 risk_estimator = 'uPU',\n",
    "                 loss = 'quadratic',\n",
    "                 max_depth = None,\n",
    "                 min_samples_leaf = 1,\n",
    "                 max_features = 'sqrt',\n",
    "                 max_candidates = 1,\n",
    "                 n_jobs = 4)\n",
    "g3.fit(P=P, U=U, pi = pi)\n",
    "predictions3 = g3.predict(X_test)\n",
    "print('Accuracy', (predictions3 == y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-regulation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
